{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "hchn3qvru4wxzmzux24v",
   "authorId": "277665006691",
   "authorName": "MIKE",
   "authorEmail": "durand.kwok@snowflake.com",
   "sessionId": "ced0bd1e-c0e6-49ac-8e57-e3b82f84d554",
   "lastEditTime": 1767211137210
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# â„ï¸ Snowflake for Hedge Funds: Finding Alpha\n\nThis notebook demonstrates how hedge funds can leverage Snowflake to:\n- **Unify data sources** (market data + alternative data)\n- **Discover alpha factors** using Snowpark Python\n- **Combine signals** for trading decisions\n\n\n---\n\n## ðŸ“š Table of Contents\n1. Setup & Connect to Snowflake\n2. Create Sample Data Tables\n3. Compute Alpha Factors (Momentum, Volatility, Sentiment)\n4. Combine Factors into Composite Alpha\n5. Analyze & Visualize Alpha Signals\n6. Create Production UDFs\n7. Backtest Strategy\n8. Deployment Patterns\n",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## 1ï¸âƒ£ Setup: Connect to Snowflake\n",
    "\n",
    "First, we establish our Snowpark session. In Snowflake Notebooks, the session is automatically available.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import (\n",
    "    col, lit, avg, sum as sum_, count, stddev, \n",
    "    lag, lead, percent_rank, row_number,\n",
    "    when, iff, greatest, least,\n",
    "    date_trunc, datediff, current_date\n",
    ")\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# In Snowflake Notebooks, session is pre-configured\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "print(f\"âœ… Connected to Snowflake\")\n",
    "print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"   Database: {session.get_current_database()}\")\n",
    "print(f\"   Schema: {session.get_current_schema()}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## 2ï¸âƒ£ Load Market Data from Snowflake Marketplace\n",
    "\n",
    "We'll use **real market data** from the **Cybersyn Financial & Economic Essentials** dataset available on Snowflake Marketplace.\n",
    "\n",
    "This demonstrates how hedge funds can instantly access high-quality market data without any ETL!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create database and schema for hedge fund analytics\n",
    "session.sql(\"CREATE DATABASE IF NOT EXISTS HEDGE_FUND_DEMO\").collect()\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS HEDGE_FUND_DEMO.ANALYTICS\").collect()\n",
    "session.sql(\"USE SCHEMA HEDGE_FUND_DEMO.ANALYTICS\").collect()\n",
    "\n",
    "print(\"âœ… Created HEDGE_FUND_DEMO.ANALYTICS schema\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load REAL market data from Cybersyn (Snowflake Marketplace)\n",
    "# This pulls historical OHLCV data for our expanded stock universe\n",
    "# Includes: Tech, Financials, Payments, Healthcare, Consumer, Energy, Industrials\n",
    "\n",
    "market_data_sql = \"\"\"\n",
    "SELECT \n",
    "    t.DATE,\n",
    "    t.TICKER AS SYMBOL,\n",
    "    MAX(CASE WHEN t.VARIABLE_NAME = 'Pre-Market Open' THEN t.VALUE END) AS OPEN,\n",
    "    MAX(CASE WHEN t.VARIABLE_NAME = 'All-Day High' THEN t.VALUE END) AS HIGH,\n",
    "    MAX(CASE WHEN t.VARIABLE_NAME = 'All-Day Low' THEN t.VALUE END) AS LOW,\n",
    "    MAX(CASE WHEN t.VARIABLE_NAME = 'Post-Market Close' THEN t.VALUE END) AS CLOSE,\n",
    "    MAX(CASE WHEN t.VARIABLE_NAME = 'Nasdaq Volume' THEN t.VALUE END) AS VOLUME\n",
    "FROM FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES t\n",
    "WHERE t.TICKER IN (\n",
    "        -- Tech Giants\n",
    "        'AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META', 'NVDA', 'TSLA',\n",
    "        -- Financials (Investment Banks)\n",
    "        'JPM', 'GS', 'MS', 'BAC', 'WFC', 'C',\n",
    "        -- Payments & FinTech\n",
    "        'V', 'MA', 'PYPL', 'SQ',\n",
    "        -- Healthcare & Pharma\n",
    "        'JNJ', 'UNH', 'PFE', 'MRK', 'ABBV',\n",
    "        -- Consumer & Retail\n",
    "        'WMT', 'COST', 'HD', 'NKE', 'SBUX',\n",
    "        -- Energy\n",
    "        'XOM', 'CVX', 'COP',\n",
    "        -- Industrials\n",
    "        'CAT', 'BA', 'UPS', 'HON'\n",
    "    )\n",
    "  AND t.VARIABLE_NAME IN ('Pre-Market Open', 'All-Day High', 'All-Day Low', 'Post-Market Close', 'Nasdaq Volume')\n",
    "  AND t.DATE >= DATEADD(year, -1, CURRENT_DATE())  -- Last 1 year of data\n",
    "GROUP BY t.DATE, t.TICKER\n",
    "HAVING CLOSE IS NOT NULL  -- Ensure we have closing prices\n",
    "ORDER BY t.TICKER, t.DATE\n",
    "\"\"\"\n",
    "\n",
    "market_df = session.sql(market_data_sql).to_pandas()\n",
    "print(f\"âœ… Loaded {len(market_df):,} market data records from Cybersyn\")\n",
    "print(f\"   Date range: {market_df['DATE'].min()} to {market_df['DATE'].max()}\")\n",
    "print(f\"   Symbols ({len(market_df['SYMBOL'].unique())} stocks): {', '.join(sorted(market_df['SYMBOL'].unique()))}\")\n",
    "market_df.head(10)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Write market data to our analytics schema\n",
    "# This creates a copy we can work with for our alpha calculations\n",
    "market_snow_df = session.create_dataframe(market_df)\n",
    "market_snow_df.write.mode(\"overwrite\").save_as_table(\"MARKET_DATA\")\n",
    "\n",
    "print(\"âœ… Created MARKET_DATA table from Cybersyn Marketplace data\")\n",
    "print(f\"   Source: FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\n",
    "session.table(\"MARKET_DATA\").show(5)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Generate sample news headlines and use SNOWFLAKE CORTEX AI for sentiment analysis!\n",
    "# This demonstrates how Cortex LLM functions can analyze text at scale\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get date range from our market data\n",
    "min_date = market_df['DATE'].min()\n",
    "max_date = market_df['DATE'].max()\n",
    "symbols = market_df['SYMBOL'].unique().tolist()\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Realistic financial news headline templates (no pre-scored sentiment!)\n",
    "headline_templates = [\n",
    "    \"beats earnings expectations with strong quarterly results\",\n",
    "    \"announces strategic partnership to expand market presence\",\n",
    "    \"faces regulatory investigation over compliance concerns\",\n",
    "    \"launches innovative new product line ahead of schedule\",\n",
    "    \"reports significant supply chain disruptions affecting production\",\n",
    "    \"receives analyst upgrade citing growth potential\",\n",
    "    \"CEO sells significant stock holdings in planned transaction\",\n",
    "    \"posts record revenue growth exceeding analyst estimates\",\n",
    "    \"misses quarterly targets amid challenging market conditions\",\n",
    "    \"expands aggressively into new international markets\",\n",
    "    \"announces major layoffs as part of restructuring plan\",\n",
    "    \"secures major government contract worth billions\",\n",
    "    \"faces class action lawsuit from shareholders\",\n",
    "    \"reports cybersecurity breach affecting customer data\",\n",
    "    \"raises full-year guidance following strong performance\",\n",
    "    \"cuts dividend amid cash flow concerns\",\n",
    "    \"announces stock buyback program worth $10 billion\",\n",
    "    \"loses key executive to competitor\",\n",
    "    \"wins patent dispute against rival company\",\n",
    "    \"warns of slowing demand in key markets\"\n",
    "]\n",
    "\n",
    "sources = ['Reuters', 'Bloomberg', 'CNBC', 'WSJ', 'Financial Times', 'MarketWatch']\n",
    "\n",
    "# Generate headlines (we'll let Cortex score them!)\n",
    "headline_records = []\n",
    "current_date = pd.to_datetime(min_date)\n",
    "end_date = pd.to_datetime(max_date)\n",
    "\n",
    "while current_date <= end_date:\n",
    "    if current_date.weekday() < 5:  # Weekdays only\n",
    "        for symbol in symbols:\n",
    "            n_articles = np.random.poisson(2)  # Average 2 articles per stock per day\n",
    "            for _ in range(n_articles):\n",
    "                headline = f\"{symbol} {random.choice(headline_templates)}\"\n",
    "                headline_records.append({\n",
    "                    'DATE': current_date.strftime('%Y-%m-%d'),\n",
    "                    'SYMBOL': symbol,\n",
    "                    'HEADLINE': headline,\n",
    "                    'SOURCE': random.choice(sources)\n",
    "                })\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "headline_df = pd.DataFrame(headline_records)\n",
    "print(f\"ðŸ“° Generated {len(headline_df):,} news headlines\")\n",
    "print(f\"   Now using Snowflake Cortex AI to analyze sentiment...\")\n",
    "\n",
    "# Write headlines to staging table\n",
    "headline_snow_df = session.create_dataframe(headline_df)\n",
    "headline_snow_df.write.mode(\"overwrite\").save_as_table(\"NEWS_HEADLINES_RAW\")\n",
    "\n",
    "# Use SNOWFLAKE CORTEX SENTIMENT function to analyze headlines!\n",
    "# This is the magic - LLM-powered sentiment analysis at scale!\n",
    "cortex_sentiment_sql = \"\"\"\n",
    "SELECT \n",
    "    DATE,\n",
    "    SYMBOL,\n",
    "    HEADLINE,\n",
    "    SOURCE,\n",
    "    -- Snowflake Cortex SENTIMENT function returns score from -1 to 1\n",
    "    SNOWFLAKE.CORTEX.SENTIMENT(HEADLINE) AS SENTIMENT_SCORE,\n",
    "    -- Confidence derived from sentiment MAGNITUDE using Cortex output\n",
    "    -- Strong sentiment (Â±0.9) = high confidence, Weak sentiment (Â±0.1) = low confidence\n",
    "    -- Formula: 0.5 + (|sentiment| * 0.5) gives range 0.5 to 1.0\n",
    "    0.5 + (ABS(SNOWFLAKE.CORTEX.SENTIMENT(HEADLINE)) * 0.5) AS CONFIDENCE\n",
    "FROM NEWS_HEADLINES_RAW\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ¤– Running Snowflake Cortex SENTIMENT analysis...\")\n",
    "session.sql(cortex_sentiment_sql).write.mode(\"overwrite\").save_as_table(\"NEWS_SENTIMENT\")\n",
    "\n",
    "print(f\"âœ… Created NEWS_SENTIMENT table with Cortex AI sentiment scores!\")\n",
    "print(f\"   ðŸ§  Powered by: SNOWFLAKE.CORTEX.SENTIMENT()\")\n",
    "print(f\"   ðŸ“Š Sentiment range: -1 (very negative) to +1 (very positive)\")\n",
    "print(f\"   ðŸŽ¯ Confidence: Derived from sentiment magnitude (stronger = more confident)\")\n",
    "\n",
    "# Show sample with Cortex-generated sentiment and confidence\n",
    "session.sql(\"\"\"\n",
    "    SELECT SYMBOL, \n",
    "           ROUND(SENTIMENT_SCORE, 3) AS SENTIMENT,\n",
    "           ROUND(CONFIDENCE, 3) AS CONFIDENCE,\n",
    "           CASE \n",
    "               WHEN SENTIMENT_SCORE > 0.3 THEN 'ðŸŸ¢ Positive'\n",
    "               WHEN SENTIMENT_SCORE < -0.3 THEN 'ðŸ”´ Negative'\n",
    "               ELSE 'ðŸŸ¡ Neutral'\n",
    "           END AS LABEL,\n",
    "           HEADLINE\n",
    "    FROM NEWS_SENTIMENT \n",
    "    ORDER BY ABS(SENTIMENT_SCORE) DESC \n",
    "    LIMIT 10\n",
    "\"\"\").show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Compute Alpha Factors with Snowflake\n",
    "\n",
    "Now we compute various alpha factors **directly in Snowflake**. This scales to billions of rows without moving data.\n",
    "\n",
    "### ðŸ“ˆ Factor 1: Momentum Alpha\n",
    "Classic momentum: stocks that have risen tend to continue rising.\n",
    "\n",
    "We use **ASOF JOIN** (Snowflake's time-series function) to find prices from ~20 trading days ago. This is more robust than `LAG()` when data has gaps.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell10",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Compute Momentum Factor using ASOF JOIN (Snowflake Time-Series)\n",
    "# ASOF JOIN finds the closest historical price by time, more robust than LAG()\n",
    "\n",
    "momentum_sql = \"\"\"\n",
    "WITH current_prices AS (\n",
    "    -- Current day prices\n",
    "    SELECT DATE, SYMBOL, CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "historical_prices AS (\n",
    "    -- Historical prices: only include dates from 20+ days ago\n",
    "    -- This ensures ASOF JOIN finds the closest match WITHIN our target range\n",
    "    SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS HIST_CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "-- Create a cross join of current dates with target lookback dates\n",
    "date_targets AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        CLOSE,\n",
    "        DATEADD('day', -28, DATE) AS TARGET_DATE  -- Target ~20 trading days (28 calendar days)\n",
    "    FROM current_prices\n",
    "),\n",
    "momentum_raw AS (\n",
    "    -- Use ASOF JOIN to find the closest price ON OR BEFORE the target date\n",
    "    SELECT \n",
    "        dt.DATE,\n",
    "        dt.SYMBOL,\n",
    "        dt.CLOSE,\n",
    "        h.HIST_DATE,\n",
    "        h.HIST_CLOSE AS CLOSE_20D_AGO,\n",
    "        DATEDIFF('day', h.HIST_DATE, dt.DATE) AS DAYS_BACK,\n",
    "        (dt.CLOSE - h.HIST_CLOSE) / NULLIF(h.HIST_CLOSE, 0) AS MOMENTUM_20D\n",
    "    FROM date_targets dt\n",
    "    ASOF JOIN historical_prices h\n",
    "        MATCH_CONDITION (dt.TARGET_DATE >= h.HIST_DATE)\n",
    "        ON dt.SYMBOL = h.SYMBOL\n",
    "),\n",
    "ranked AS (\n",
    "    -- Rank stocks by momentum on each date (cross-sectional)\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        CLOSE,\n",
    "        CLOSE_20D_AGO,\n",
    "        DAYS_BACK,\n",
    "        MOMENTUM_20D,\n",
    "        PERCENT_RANK() OVER (\n",
    "            PARTITION BY DATE \n",
    "            ORDER BY MOMENTUM_20D\n",
    "        ) AS MOMENTUM_RANK\n",
    "    FROM momentum_raw\n",
    "    WHERE MOMENTUM_20D IS NOT NULL\n",
    "      AND DAYS_BACK >= 20  -- Ensure we have enough history\n",
    ")\n",
    "-- Generate trading signals\n",
    "SELECT \n",
    "    DATE,\n",
    "    SYMBOL,\n",
    "    CLOSE,\n",
    "    MOMENTUM_20D,\n",
    "    MOMENTUM_RANK,\n",
    "    CASE \n",
    "        WHEN MOMENTUM_RANK > 0.8 THEN 1   -- Top 20% = BUY\n",
    "        WHEN MOMENTUM_RANK < 0.2 THEN -1  -- Bottom 20% = SELL\n",
    "        ELSE 0                             -- Middle = HOLD\n",
    "    END AS MOMENTUM_SIGNAL\n",
    "FROM ranked\n",
    "ORDER BY DATE, SYMBOL\n",
    "\"\"\"\n",
    "\n",
    "momentum_df = session.sql(momentum_sql)\n",
    "momentum_df.write.mode(\"overwrite\").save_as_table(\"MOMENTUM_FACTOR\")\n",
    "\n",
    "print(\"âœ… Computed Momentum Alpha using ASOF JOIN\")\n",
    "print(\"   (Time-series function finds closest price ~20 trading days ago)\")\n",
    "session.table(\"MOMENTUM_FACTOR\").show(10)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell11"
   },
   "source": [
    "### ðŸ“‰ Factor 2: Volatility Alpha\n",
    "Low volatility anomaly: less volatile stocks often outperform on risk-adjusted basis.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell12",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Compute Volatility Factor using ASOF JOIN (Snowflake Time-Series)\n",
    "# ASOF JOIN is more robust than LAG() for handling gaps in data (holidays, missing days)\n",
    "\n",
    "volatility_sql = \"\"\"\n",
    "WITH current_prices AS (\n",
    "    -- Today's prices\n",
    "    SELECT DATE, SYMBOL, CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "historical_prices AS (\n",
    "    -- Historical prices for ASOF JOIN\n",
    "    SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS PREV_CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "-- Step 1: Use ASOF JOIN to find previous day's price (handles gaps!)\n",
    "daily_returns AS (\n",
    "    SELECT \n",
    "        c.DATE,\n",
    "        c.SYMBOL,\n",
    "        c.CLOSE,\n",
    "        h.HIST_DATE AS PREV_DATE,\n",
    "        h.PREV_CLOSE,\n",
    "        DATEDIFF('day', h.HIST_DATE, c.DATE) AS DAYS_GAP,\n",
    "        -- Daily return = (today - yesterday) / yesterday\n",
    "        (c.CLOSE - h.PREV_CLOSE) / NULLIF(h.PREV_CLOSE, 0) AS DAILY_RETURN\n",
    "    FROM current_prices c\n",
    "    ASOF JOIN historical_prices h\n",
    "        MATCH_CONDITION (c.DATE > h.HIST_DATE)  -- Find the most recent date BEFORE today\n",
    "        ON c.SYMBOL = h.SYMBOL\n",
    "    WHERE h.HIST_DATE >= DATEADD('day', -5, c.DATE)  -- Within 5 days (handles weekends)\n",
    "),\n",
    "-- Step 2: Calculate rolling 20-day volatility (annualized)\n",
    "volatility AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        CLOSE,\n",
    "        -- STDDEV over last 20 trading days, annualized by âˆš252\n",
    "        STDDEV(DAILY_RETURN) OVER (\n",
    "            PARTITION BY SYMBOL \n",
    "            ORDER BY DATE \n",
    "            ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "        ) * SQRT(252) AS VOLATILITY_20D,\n",
    "        -- Count how many days we have (data quality check)\n",
    "        COUNT(*) OVER (\n",
    "            PARTITION BY SYMBOL \n",
    "            ORDER BY DATE \n",
    "            ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "        ) AS DAYS_IN_WINDOW\n",
    "    FROM daily_returns\n",
    "    WHERE DAILY_RETURN IS NOT NULL\n",
    "),\n",
    "-- Step 3: Rank volatility cross-sectionally (low vol = high rank)\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        CLOSE,\n",
    "        VOLATILITY_20D,\n",
    "        PERCENT_RANK() OVER (\n",
    "            PARTITION BY DATE \n",
    "            ORDER BY VOLATILITY_20D DESC  -- DESC so low vol = high rank\n",
    "        ) AS VOLATILITY_RANK\n",
    "    FROM volatility\n",
    "    WHERE VOLATILITY_20D IS NOT NULL\n",
    "      AND DAYS_IN_WINDOW >= 15  -- Need at least 15 days of data\n",
    ")\n",
    "-- Step 4: Generate trading signal\n",
    "SELECT \n",
    "    DATE,\n",
    "    SYMBOL,\n",
    "    CLOSE,\n",
    "    VOLATILITY_20D,\n",
    "    VOLATILITY_RANK,\n",
    "    CASE \n",
    "        WHEN VOLATILITY_RANK > 0.8 THEN 1   -- Lowest 20% vol = BUY\n",
    "        WHEN VOLATILITY_RANK < 0.2 THEN -1  -- Highest 20% vol = SELL\n",
    "        ELSE 0                               -- Middle = HOLD\n",
    "    END AS VOLATILITY_SIGNAL\n",
    "FROM ranked\n",
    "ORDER BY DATE, SYMBOL\n",
    "\"\"\"\n",
    "\n",
    "volatility_df = session.sql(volatility_sql)\n",
    "volatility_df.write.mode(\"overwrite\").save_as_table(\"VOLATILITY_FACTOR\")\n",
    "\n",
    "print(\"âœ… Computed Volatility Alpha using ASOF JOIN\")\n",
    "print(\"   (Time-series function finds previous trading day, handles gaps)\")\n",
    "session.table(\"VOLATILITY_FACTOR\").show(10)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell13"
   },
   "source": [
    "### ðŸ’¬ Factor 3: Sentiment Alpha\n",
    "Aggregate news sentiment as a trading signal. This data would come from **Snowflake Marketplace** providers like RavenPack.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell14",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Compute Sentiment Factor\n",
    "sentiment = session.table(\"NEWS_SENTIMENT\")\n",
    "\n",
    "# Aggregate daily sentiment\n",
    "daily_sentiment = sentiment.group_by(\"DATE\", \"SYMBOL\").agg(\n",
    "    avg(\"SENTIMENT_SCORE\").alias(\"AVG_SENTIMENT\"),\n",
    "    count(\"*\").alias(\"ARTICLE_COUNT\"),\n",
    "    avg(\"CONFIDENCE\").alias(\"AVG_CONFIDENCE\")\n",
    ")\n",
    "\n",
    "# Rank sentiment cross-sectionally\n",
    "sentiment_alpha = daily_sentiment.with_column(\n",
    "    \"SENTIMENT_RANK\",\n",
    "    percent_rank().over(Window.partition_by(\"DATE\").order_by(col(\"AVG_SENTIMENT\")))\n",
    ").with_column(\n",
    "    \"SENTIMENT_SIGNAL\",\n",
    "    when(col(\"SENTIMENT_RANK\") > 0.8, 1)\n",
    "    .when(col(\"SENTIMENT_RANK\") < 0.2, -1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "sentiment_alpha.write.mode(\"overwrite\").save_as_table(\"SENTIMENT_FACTOR\")\n",
    "\n",
    "print(\"âœ… Computed Sentiment Alpha\")\n",
    "session.table(\"SENTIMENT_FACTOR\").show(10)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Combine Alpha Factors\n",
    "\n",
    "Join all factors and create a **composite alpha signal** for trading decisions.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell16",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Combine all factors into composite alpha signal\n",
    "# Using IC-OPTIMIZED weights that FLIP negative factors!\n",
    "\n",
    "# IC-based weights: Negative IC = Flip the signal\n",
    "# Based on typical market analysis:\n",
    "#   - Momentum often reverses (mean reversion) â†’ negative weight\n",
    "#   - Low-vol anomaly may not hold in risk-on markets â†’ negative weight\n",
    "#   - Sentiment tends to be predictive â†’ positive weight\n",
    "# These will be validated/updated by the IC analysis later in the notebook\n",
    "\n",
    "MOMENTUM_WEIGHT = -0.20    # Flipped! (mean reversion works better)\n",
    "VOLATILITY_WEIGHT = -0.30  # Flipped! (high-vol outperforming in current regime)\n",
    "SENTIMENT_WEIGHT = 0.50    # Increased! (most predictive factor)\n",
    "\n",
    "print(\"ðŸ“Š Using IC-OPTIMIZED Weights:\")\n",
    "print(f\"   Momentum:   {MOMENTUM_WEIGHT:+.0%} {'(FLIPPED - betting on mean reversion)' if MOMENTUM_WEIGHT < 0 else ''}\")\n",
    "print(f\"   Volatility: {VOLATILITY_WEIGHT:+.0%} {'(FLIPPED - high-vol stocks outperforming)' if VOLATILITY_WEIGHT < 0 else ''}\")\n",
    "print(f\"   Sentiment:  {SENTIMENT_WEIGHT:+.0%} (primary signal)\")\n",
    "print()\n",
    "\n",
    "composite_alpha_sql = f\"\"\"\n",
    "WITH combined AS (\n",
    "    SELECT \n",
    "        m.DATE,\n",
    "        m.SYMBOL,\n",
    "        m.CLOSE,\n",
    "        m.MOMENTUM_RANK,\n",
    "        m.MOMENTUM_SIGNAL,\n",
    "        v.VOLATILITY_RANK,\n",
    "        v.VOLATILITY_SIGNAL,\n",
    "        COALESCE(s.SENTIMENT_RANK, 0.5) AS SENTIMENT_RANK,\n",
    "        COALESCE(s.SENTIMENT_SIGNAL, 0) AS SENTIMENT_SIGNAL,\n",
    "        s.AVG_SENTIMENT,\n",
    "        s.ARTICLE_COUNT\n",
    "    FROM MOMENTUM_FACTOR m\n",
    "    LEFT JOIN VOLATILITY_FACTOR v \n",
    "        ON m.DATE = v.DATE AND m.SYMBOL = v.SYMBOL\n",
    "    LEFT JOIN SENTIMENT_FACTOR s \n",
    "        ON m.DATE = s.DATE AND m.SYMBOL = s.SYMBOL\n",
    ")\n",
    "SELECT \n",
    "    *,\n",
    "    -- IC-OPTIMIZED composite alpha (negative weights FLIP the signal!)\n",
    "    (MOMENTUM_RANK * {MOMENTUM_WEIGHT} + \n",
    "     VOLATILITY_RANK * {VOLATILITY_WEIGHT} + \n",
    "     SENTIMENT_RANK * {SENTIMENT_WEIGHT}) AS COMPOSITE_ALPHA,\n",
    "    \n",
    "    -- Weighted signal (negative weights flip BUY to SELL)\n",
    "    (MOMENTUM_SIGNAL * {MOMENTUM_WEIGHT} + \n",
    "     VOLATILITY_SIGNAL * {VOLATILITY_WEIGHT} + \n",
    "     SENTIMENT_SIGNAL * {SENTIMENT_WEIGHT}) AS WEIGHTED_SIGNAL,\n",
    "     \n",
    "    -- Trading recommendation based on optimized signal\n",
    "    CASE \n",
    "        WHEN (MOMENTUM_SIGNAL * {MOMENTUM_WEIGHT} + VOLATILITY_SIGNAL * {VOLATILITY_WEIGHT} + SENTIMENT_SIGNAL * {SENTIMENT_WEIGHT}) > 0.2 THEN 'STRONG_BUY'\n",
    "        WHEN (MOMENTUM_SIGNAL * {MOMENTUM_WEIGHT} + VOLATILITY_SIGNAL * {VOLATILITY_WEIGHT} + SENTIMENT_SIGNAL * {SENTIMENT_WEIGHT}) > 0.05 THEN 'BUY'\n",
    "        WHEN (MOMENTUM_SIGNAL * {MOMENTUM_WEIGHT} + VOLATILITY_SIGNAL * {VOLATILITY_WEIGHT} + SENTIMENT_SIGNAL * {SENTIMENT_WEIGHT}) < -0.2 THEN 'STRONG_SELL'\n",
    "        WHEN (MOMENTUM_SIGNAL * {MOMENTUM_WEIGHT} + VOLATILITY_SIGNAL * {VOLATILITY_WEIGHT} + SENTIMENT_SIGNAL * {SENTIMENT_WEIGHT}) < -0.05 THEN 'SELL'\n",
    "        ELSE 'HOLD'\n",
    "    END AS TRADING_SIGNAL\n",
    "FROM combined\n",
    "WHERE DATE IS NOT NULL\n",
    "ORDER BY DATE DESC, COMPOSITE_ALPHA DESC\n",
    "\"\"\"\n",
    "\n",
    "composite_alpha = session.sql(composite_alpha_sql)\n",
    "composite_alpha.write.mode(\"overwrite\").save_as_table(\"ALPHA_SIGNALS\")\n",
    "\n",
    "print(\"âœ… Created ALPHA_SIGNALS table with IC-OPTIMIZED composite alpha\")\n",
    "print(\"   (Negative weights flip momentum/volatility signals for mean reversion)\")\n",
    "session.table(\"ALPHA_SIGNALS\").show(15)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "---\n",
    "## 5ï¸âƒ£ Analyze Alpha Signals\n",
    "\n",
    "Identify today's trading opportunities based on our composite alpha.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell18",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Get latest signals - Today's trading recommendations\n",
    "latest_signals_sql = \"\"\"\n",
    "SELECT \n",
    "    SYMBOL,\n",
    "    ROUND(CLOSE, 2) AS PRICE,\n",
    "    ROUND(MOMENTUM_RANK, 3) AS MOMENTUM,\n",
    "    ROUND(VOLATILITY_RANK, 3) AS VOLATILITY,\n",
    "    ROUND(SENTIMENT_RANK, 3) AS SENTIMENT,\n",
    "    ROUND(COMPOSITE_ALPHA, 3) AS ALPHA_SCORE,\n",
    "    TRADING_SIGNAL\n",
    "FROM ALPHA_SIGNALS\n",
    "WHERE DATE = (SELECT MAX(DATE) FROM ALPHA_SIGNALS)\n",
    "ORDER BY COMPOSITE_ALPHA DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸŽ¯ TODAY'S ALPHA SIGNALS\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(latest_signals_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell19",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Top Buy and Sell opportunities\n",
    "top_signals_sql = \"\"\"\n",
    "WITH latest AS (\n",
    "    SELECT * FROM ALPHA_SIGNALS\n",
    "    WHERE DATE = (SELECT MAX(DATE) FROM ALPHA_SIGNALS)\n",
    "),\n",
    "top_buys AS (\n",
    "    SELECT 'ðŸŸ¢ TOP BUYS' AS CATEGORY, SYMBOL, ROUND(COMPOSITE_ALPHA, 3) AS ALPHA, TRADING_SIGNAL\n",
    "    FROM latest \n",
    "    WHERE WEIGHTED_SIGNAL > 0\n",
    "    ORDER BY COMPOSITE_ALPHA DESC \n",
    "    LIMIT 5\n",
    "),\n",
    "top_sells AS (\n",
    "    SELECT 'ðŸ”´ TOP SELLS' AS CATEGORY, SYMBOL, ROUND(COMPOSITE_ALPHA, 3) AS ALPHA, TRADING_SIGNAL\n",
    "    FROM latest \n",
    "    WHERE WEIGHTED_SIGNAL < 0\n",
    "    ORDER BY COMPOSITE_ALPHA ASC \n",
    "    LIMIT 5\n",
    ")\n",
    "SELECT * FROM top_buys\n",
    "UNION ALL\n",
    "SELECT * FROM top_sells\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š TOP TRADING OPPORTUNITIES\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(top_signals_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell20",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Signal distribution summary\n",
    "signal_summary_sql = \"\"\"\n",
    "SELECT \n",
    "    TRADING_SIGNAL,\n",
    "    COUNT(*) AS COUNT,\n",
    "    ROUND(AVG(COMPOSITE_ALPHA), 3) AS AVG_ALPHA,\n",
    "    ROUND(AVG(MOMENTUM_RANK), 3) AS AVG_MOMENTUM,\n",
    "    ROUND(AVG(SENTIMENT_RANK), 3) AS AVG_SENTIMENT\n",
    "FROM ALPHA_SIGNALS\n",
    "WHERE DATE = (SELECT MAX(DATE) FROM ALPHA_SIGNALS)\n",
    "GROUP BY TRADING_SIGNAL\n",
    "ORDER BY AVG_ALPHA DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ˆ SIGNAL DISTRIBUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(signal_summary_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "---\n",
    "## 6ï¸âƒ£ Visualize Results\n",
    "\n",
    "Create charts to visualize alpha signals and factor performance.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell22",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load data for visualization\n",
    "# Note: Snowflake Notebooks uses Streamlit for rendering\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "alpha_data = session.table(\"ALPHA_SIGNALS\").to_pandas()\n",
    "alpha_data['DATE'] = pd.to_datetime(alpha_data['DATE'])\n",
    "\n",
    "print(f\"Loaded {len(alpha_data):,} records for visualization\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Alpha Score Distribution\n",
    "fig = px.histogram(\n",
    "    alpha_data, \n",
    "    x='COMPOSITE_ALPHA',\n",
    "    nbins=50,\n",
    "    title='ðŸ“Š Distribution of Composite Alpha Scores',\n",
    "    color_discrete_sequence=['#29B5E8']\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title='Alpha Score',\n",
    "    yaxis_title='Frequency',\n",
    "    template='plotly_white'\n",
    ")\n",
    "st.plotly_chart(fig, use_container_width=True)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell24",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Alpha by Symbol (latest date)\n",
    "latest_date = alpha_data['DATE'].max()\n",
    "latest_alpha = alpha_data[alpha_data['DATE'] == latest_date].sort_values('COMPOSITE_ALPHA', ascending=False)\n",
    "\n",
    "fig = px.bar(\n",
    "    latest_alpha,\n",
    "    x='SYMBOL',\n",
    "    y='COMPOSITE_ALPHA',\n",
    "    color='TRADING_SIGNAL',\n",
    "    color_discrete_map={\n",
    "        'STRONG_BUY': '#00C853',\n",
    "        'BUY': '#29B5E8',\n",
    "        'HOLD': '#888888',\n",
    "        'SELL': '#FF9800',\n",
    "        'STRONG_SELL': '#FF1744'\n",
    "    },\n",
    "    title=f'ðŸŽ¯ Alpha Scores by Symbol ({latest_date.strftime(\"%Y-%m-%d\")})'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Symbol', yaxis_title='Composite Alpha')\n",
    "st.plotly_chart(fig, use_container_width=True)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell25",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Factor Correlation Heatmap\n",
    "factor_cols = ['MOMENTUM_RANK', 'VOLATILITY_RANK', 'SENTIMENT_RANK', 'COMPOSITE_ALPHA']\n",
    "corr_matrix = alpha_data[factor_cols].corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    labels=dict(color=\"Correlation\"),\n",
    "    x=['Momentum', 'Volatility', 'Sentiment', 'Composite'],\n",
    "    y=['Momentum', 'Volatility', 'Sentiment', 'Composite'],\n",
    "    color_continuous_scale='RdBu',\n",
    "    title='ðŸ”— Factor Correlation Matrix'\n",
    ")\n",
    "st.plotly_chart(fig, use_container_width=True)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell26"
   },
   "source": [
    "---\n",
    "## 7ï¸âƒ£ Backtest Alpha Strategy\n",
    "\n",
    "Calculate the historical performance of our alpha signals.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell27",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Calculate forward returns and strategy performance\n",
    "# Using WEIGHTED_SIGNAL (works correctly with optimized weights)\n",
    "\n",
    "backtest_sql = \"\"\"\n",
    "WITH signals_with_returns AS (\n",
    "    SELECT \n",
    "        a.*,\n",
    "        LEAD(CLOSE, 5) OVER (PARTITION BY SYMBOL ORDER BY DATE) AS CLOSE_5D_FORWARD,\n",
    "        (LEAD(CLOSE, 5) OVER (PARTITION BY SYMBOL ORDER BY DATE) - CLOSE) / CLOSE AS FORWARD_RETURN_5D\n",
    "    FROM ALPHA_SIGNALS a\n",
    "),\n",
    "strategy_returns AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        WEIGHTED_SIGNAL,\n",
    "        TRADING_SIGNAL,\n",
    "        FORWARD_RETURN_5D,\n",
    "        -- Use WEIGHTED_SIGNAL instead of COMPOSITE_ALPHA thresholds\n",
    "        -- Positive signal = LONG, Negative signal = SHORT\n",
    "        CASE \n",
    "            WHEN WEIGHTED_SIGNAL > 0 THEN FORWARD_RETURN_5D      -- LONG position\n",
    "            WHEN WEIGHTED_SIGNAL < 0 THEN -FORWARD_RETURN_5D     -- SHORT position\n",
    "            ELSE 0                                                -- No trade\n",
    "        END AS STRATEGY_RETURN\n",
    "    FROM signals_with_returns\n",
    "    WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    'IC-Optimized Alpha Strategy' AS STRATEGY,\n",
    "    COUNT(*) AS N_TRADES,\n",
    "    ROUND(AVG(STRATEGY_RETURN) * 100, 3) AS AVG_RETURN_PCT,\n",
    "    ROUND(STDDEV(STRATEGY_RETURN) * 100, 3) AS STD_DEV_PCT,\n",
    "    ROUND(AVG(STRATEGY_RETURN) / NULLIF(STDDEV(STRATEGY_RETURN), 0) * SQRT(52), 2) AS SHARPE_RATIO,\n",
    "    ROUND(SUM(CASE WHEN STRATEGY_RETURN > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS HIT_RATE_PCT\n",
    "FROM strategy_returns\n",
    "WHERE STRATEGY_RETURN != 0\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ˆ BACKTEST RESULTS (IC-Optimized Strategy)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Strategy: LONG when WEIGHTED_SIGNAL > 0, SHORT when < 0\")\n",
    "print(\"Weights: Momentum -20%, Volatility -30%, Sentiment +50%\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(backtest_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell28"
   },
   "source": [
    "### ðŸ“Š Long-Short Equity Analysis\n",
    "\n",
    "First, let's examine the **current portfolio composition** - which stocks are we long vs short, and what's our sector exposure?\n",
    "\n",
    "This shows the **real-time trading signals** from our alpha model.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell29",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Long-Short Equity Analysis\n",
    "# Shows current portfolio positions and sector exposure\n",
    "\n",
    "# Define sector mappings\n",
    "sector_map_sql = \"\"\"\n",
    "SELECT 'AAPL' AS SYMBOL, 'Tech' AS SECTOR UNION ALL\n",
    "SELECT 'GOOGL', 'Tech' UNION ALL SELECT 'MSFT', 'Tech' UNION ALL\n",
    "SELECT 'AMZN', 'Tech' UNION ALL SELECT 'META', 'Tech' UNION ALL\n",
    "SELECT 'NVDA', 'Tech' UNION ALL SELECT 'TSLA', 'Tech' UNION ALL\n",
    "SELECT 'JPM', 'Financials' UNION ALL SELECT 'GS', 'Financials' UNION ALL\n",
    "SELECT 'MS', 'Financials' UNION ALL SELECT 'BAC', 'Financials' UNION ALL\n",
    "SELECT 'WFC', 'Financials' UNION ALL SELECT 'C', 'Financials' UNION ALL\n",
    "SELECT 'V', 'Payments' UNION ALL SELECT 'MA', 'Payments' UNION ALL\n",
    "SELECT 'PYPL', 'Payments' UNION ALL SELECT 'SQ', 'Payments' UNION ALL\n",
    "SELECT 'JNJ', 'Healthcare' UNION ALL SELECT 'UNH', 'Healthcare' UNION ALL\n",
    "SELECT 'PFE', 'Healthcare' UNION ALL SELECT 'MRK', 'Healthcare' UNION ALL\n",
    "SELECT 'ABBV', 'Healthcare' UNION ALL\n",
    "SELECT 'WMT', 'Consumer' UNION ALL SELECT 'COST', 'Consumer' UNION ALL\n",
    "SELECT 'HD', 'Consumer' UNION ALL SELECT 'NKE', 'Consumer' UNION ALL\n",
    "SELECT 'SBUX', 'Consumer' UNION ALL\n",
    "SELECT 'XOM', 'Energy' UNION ALL SELECT 'CVX', 'Energy' UNION ALL\n",
    "SELECT 'COP', 'Energy' UNION ALL\n",
    "SELECT 'CAT', 'Industrials' UNION ALL SELECT 'BA', 'Industrials' UNION ALL\n",
    "SELECT 'UPS', 'Industrials' UNION ALL SELECT 'HON', 'Industrials'\n",
    "\"\"\"\n",
    "\n",
    "# Current Long-Short Portfolio Holdings\n",
    "holdings_sql = f\"\"\"\n",
    "WITH sectors AS ({sector_map_sql}),\n",
    "latest_signals AS (\n",
    "    SELECT \n",
    "        a.SYMBOL,\n",
    "        a.CLOSE AS PRICE,\n",
    "        a.WEIGHTED_SIGNAL,\n",
    "        a.TRADING_SIGNAL,\n",
    "        a.COMPOSITE_ALPHA,\n",
    "        a.MOMENTUM_RANK,\n",
    "        a.VOLATILITY_RANK,\n",
    "        a.SENTIMENT_RANK,\n",
    "        s.SECTOR,\n",
    "        CASE \n",
    "            WHEN a.WEIGHTED_SIGNAL > 0 THEN 'LONG'\n",
    "            WHEN a.WEIGHTED_SIGNAL < 0 THEN 'SHORT'\n",
    "            ELSE 'NO_POSITION'\n",
    "        END AS POSITION\n",
    "    FROM ALPHA_SIGNALS a\n",
    "    LEFT JOIN sectors s ON a.SYMBOL = s.SYMBOL\n",
    "    WHERE a.DATE = (SELECT MAX(DATE) FROM ALPHA_SIGNALS)\n",
    ")\n",
    "SELECT * FROM latest_signals\n",
    "WHERE POSITION != 'NO_POSITION'\n",
    "ORDER BY POSITION, COMPOSITE_ALPHA DESC\n",
    "\"\"\"\n",
    "\n",
    "holdings_df = session.sql(holdings_sql).to_pandas()\n",
    "\n",
    "# Separate long and short positions\n",
    "longs = holdings_df[holdings_df['POSITION'] == 'LONG']\n",
    "shorts = holdings_df[holdings_df['POSITION'] == 'SHORT']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“Š LONG-SHORT EQUITY PORTFOLIO ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸŸ¢ LONG POSITIONS ({len(longs)} stocks)\")\n",
    "print(\"-\" * 60)\n",
    "if len(longs) > 0:\n",
    "    print(f\"{'Symbol':<8} {'Sector':<12} {'Price':>10} {'Alpha':>8} {'Signal':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in longs.iterrows():\n",
    "        print(f\"{row['SYMBOL']:<8} {row['SECTOR']:<12} ${row['PRICE']:>8.2f} {row['COMPOSITE_ALPHA']:>+8.3f} {row['TRADING_SIGNAL']:>12}\")\n",
    "\n",
    "print(f\"\\nðŸ”´ SHORT POSITIONS ({len(shorts)} stocks)\")\n",
    "print(\"-\" * 60)\n",
    "if len(shorts) > 0:\n",
    "    print(f\"{'Symbol':<8} {'Sector':<12} {'Price':>10} {'Alpha':>8} {'Signal':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in shorts.iterrows():\n",
    "        print(f\"{row['SYMBOL']:<8} {row['SECTOR']:<12} ${row['PRICE']:>8.2f} {row['COMPOSITE_ALPHA']:>+8.3f} {row['TRADING_SIGNAL']:>12}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell30",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Sector Exposure Analysis\n",
    "# This is critical for hedge funds to manage concentration risk\n",
    "\n",
    "sector_exposure_sql = f\"\"\"\n",
    "WITH sectors AS ({sector_map_sql}),\n",
    "latest_signals AS (\n",
    "    SELECT \n",
    "        a.SYMBOL,\n",
    "        a.WEIGHTED_SIGNAL,\n",
    "        s.SECTOR,\n",
    "        CASE \n",
    "            WHEN a.WEIGHTED_SIGNAL > 0 THEN 'LONG'\n",
    "            WHEN a.WEIGHTED_SIGNAL < 0 THEN 'SHORT'\n",
    "            ELSE 'NEUTRAL'\n",
    "        END AS POSITION\n",
    "    FROM ALPHA_SIGNALS a\n",
    "    LEFT JOIN sectors s ON a.SYMBOL = s.SYMBOL\n",
    "    WHERE a.DATE = (SELECT MAX(DATE) FROM ALPHA_SIGNALS)\n",
    ")\n",
    "SELECT \n",
    "    SECTOR,\n",
    "    SUM(CASE WHEN POSITION = 'LONG' THEN 1 ELSE 0 END) AS LONG_COUNT,\n",
    "    SUM(CASE WHEN POSITION = 'SHORT' THEN 1 ELSE 0 END) AS SHORT_COUNT,\n",
    "    SUM(CASE WHEN POSITION = 'LONG' THEN 1 ELSE 0 END) - \n",
    "    SUM(CASE WHEN POSITION = 'SHORT' THEN 1 ELSE 0 END) AS NET_EXPOSURE\n",
    "FROM latest_signals\n",
    "GROUP BY SECTOR\n",
    "ORDER BY NET_EXPOSURE DESC\n",
    "\"\"\"\n",
    "\n",
    "sector_df = session.sql(sector_exposure_sql).to_pandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ­ SECTOR EXPOSURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Sector':<15} {'Longs':>8} {'Shorts':>8} {'Net':>8} {'Exposure Bar':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for _, row in sector_df.iterrows():\n",
    "    net = row['NET_EXPOSURE']\n",
    "    bar_len = min(abs(net), 10)\n",
    "    if net > 0:\n",
    "        bar = \"ðŸŸ¢\" * bar_len\n",
    "        exposure = \"NET LONG\"\n",
    "    elif net < 0:\n",
    "        bar = \"ðŸ”´\" * bar_len\n",
    "        exposure = \"NET SHORT\"\n",
    "    else:\n",
    "        bar = \"âšª\"\n",
    "        exposure = \"NEUTRAL\"\n",
    "    print(f\"{row['SECTOR']:<15} {row['LONG_COUNT']:>8} {row['SHORT_COUNT']:>8} {net:>+8} {bar}\")\n",
    "\n",
    "# Portfolio Summary\n",
    "total_longs = len(longs)\n",
    "total_shorts = len(shorts)\n",
    "gross_exposure = total_longs + total_shorts\n",
    "net_exposure = total_longs - total_shorts\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“ˆ PORTFOLIO SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   â€¢ Long Positions:   {total_longs} stocks\")\n",
    "print(f\"   â€¢ Short Positions:  {total_shorts} stocks\")\n",
    "print(f\"   â€¢ Gross Exposure:   {gross_exposure} positions\")\n",
    "print(f\"   â€¢ Net Exposure:     {net_exposure:+d} ({'Long Bias' if net_exposure > 0 else 'Short Bias' if net_exposure < 0 else 'Market Neutral'})\")\n",
    "print(f\"   â€¢ Long/Short Ratio: {total_longs/max(total_shorts,1):.2f}x\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell31",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Visualize Long-Short Portfolio by Sector\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create side-by-side bar chart\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=('Portfolio Positions by Sector', 'Long vs Short Distribution'),\n",
    "                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}]])\n",
    "\n",
    "# Bar chart: Sector exposure\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Long', x=sector_df['SECTOR'], y=sector_df['LONG_COUNT'], \n",
    "           marker_color='#00C853'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Short', x=sector_df['SECTOR'], y=-sector_df['SHORT_COUNT'], \n",
    "           marker_color='#FF1744'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Pie chart: Long vs Short split\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=['Long Positions', 'Short Positions'], \n",
    "           values=[total_longs, total_shorts],\n",
    "           marker_colors=['#00C853', '#FF1744'],\n",
    "           hole=0.4),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='ðŸ“Š Long-Short Equity Portfolio Composition',\n",
    "    barmode='relative',\n",
    "    height=400\n",
    ")\n",
    "fig.update_yaxes(title_text=\"# of Positions\", row=1, col=1)\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Show the actual holdings table\n",
    "print(\"\\nðŸ“‹ DETAILED HOLDINGS:\")\n",
    "session.sql(holdings_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell32"
   },
   "source": [
    "### ðŸ“ˆðŸ“‰ Long-Short Portfolio Backtest\n",
    "\n",
    "Now let's see how this strategy would have performed **historically**.\n",
    "\n",
    "This is how hedge funds actually trade: **LONG** the winners, **SHORT** the losers, and capture the **spread**.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell33",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Long-Short Portfolio Backtest\n",
    "# LONG top 20% (signal = +1), SHORT bottom 20% (signal = -1)\n",
    "\n",
    "long_short_sql = \"\"\"\n",
    "WITH signals_with_returns AS (\n",
    "    -- Get forward 5-day returns for each signal\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        TRADING_SIGNAL,\n",
    "        COMPOSITE_ALPHA,\n",
    "        WEIGHTED_SIGNAL,\n",
    "        CLOSE,\n",
    "        LEAD(CLOSE, 5) OVER (PARTITION BY SYMBOL ORDER BY DATE) AS CLOSE_5D_FORWARD,\n",
    "        (LEAD(CLOSE, 5) OVER (PARTITION BY SYMBOL ORDER BY DATE) - CLOSE) / CLOSE AS FORWARD_RETURN_5D\n",
    "    FROM ALPHA_SIGNALS\n",
    "),\n",
    "daily_returns AS (\n",
    "    -- Calculate daily long and short portfolio returns\n",
    "    SELECT \n",
    "        DATE,\n",
    "        -- Long portfolio: average return of stocks with positive signal\n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL > 0 THEN FORWARD_RETURN_5D END) AS LONG_RETURN,\n",
    "        COUNT(CASE WHEN WEIGHTED_SIGNAL > 0 THEN 1 END) AS LONG_COUNT,\n",
    "        -- Short portfolio: average return of stocks with negative signal  \n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL < 0 THEN FORWARD_RETURN_5D END) AS SHORT_RETURN,\n",
    "        COUNT(CASE WHEN WEIGHTED_SIGNAL < 0 THEN 1 END) AS SHORT_COUNT,\n",
    "        -- Long-Short spread (this is the hedge fund return!)\n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL > 0 THEN FORWARD_RETURN_5D END) - \n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL < 0 THEN FORWARD_RETURN_5D END) AS LONG_SHORT_RETURN\n",
    "    FROM signals_with_returns\n",
    "    WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    "    GROUP BY DATE\n",
    "    HAVING LONG_COUNT > 0 AND SHORT_COUNT > 0\n",
    ")\n",
    "SELECT \n",
    "    'Long Portfolio (Buy Winners)' AS PORTFOLIO,\n",
    "    ROUND(AVG(LONG_RETURN) * 100, 3) AS AVG_RETURN_PCT,\n",
    "    ROUND(STDDEV(LONG_RETURN) * 100, 3) AS VOLATILITY_PCT,\n",
    "    ROUND(AVG(LONG_RETURN) / NULLIF(STDDEV(LONG_RETURN), 0) * SQRT(52), 2) AS SHARPE,\n",
    "    ROUND(SUM(CASE WHEN LONG_RETURN > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS WIN_RATE\n",
    "FROM daily_returns\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Short Portfolio (Sell Losers)' AS PORTFOLIO,\n",
    "    ROUND(AVG(-SHORT_RETURN) * 100, 3) AS AVG_RETURN_PCT,  -- Negative because we're short\n",
    "    ROUND(STDDEV(SHORT_RETURN) * 100, 3) AS VOLATILITY_PCT,\n",
    "    ROUND(AVG(-SHORT_RETURN) / NULLIF(STDDEV(SHORT_RETURN), 0) * SQRT(52), 2) AS SHARPE,\n",
    "    ROUND(SUM(CASE WHEN SHORT_RETURN < 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS WIN_RATE\n",
    "FROM daily_returns\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'ðŸ“Š LONG-SHORT SPREAD (Hedge Fund Return)' AS PORTFOLIO,\n",
    "    ROUND(AVG(LONG_SHORT_RETURN) * 100, 3) AS AVG_RETURN_PCT,\n",
    "    ROUND(STDDEV(LONG_SHORT_RETURN) * 100, 3) AS VOLATILITY_PCT,\n",
    "    ROUND(AVG(LONG_SHORT_RETURN) / NULLIF(STDDEV(LONG_SHORT_RETURN), 0) * SQRT(52), 2) AS SHARPE,\n",
    "    ROUND(SUM(CASE WHEN LONG_SHORT_RETURN > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS WIN_RATE\n",
    "FROM daily_returns\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ˆðŸ“‰ LONG-SHORT PORTFOLIO BACKTEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Strategy: LONG top 20% momentum, SHORT bottom 20% momentum\")\n",
    "print(\"Holding Period: 5 days\")\n",
    "print(\"=\" * 70)\n",
    "session.sql(long_short_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell34",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Visualize Cumulative Long-Short Returns Over Time\n",
    "cumulative_sql = \"\"\"\n",
    "WITH signals_with_returns AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        WEIGHTED_SIGNAL,\n",
    "        CLOSE,\n",
    "        (LEAD(CLOSE, 5) OVER (PARTITION BY SYMBOL ORDER BY DATE) - CLOSE) / CLOSE AS FORWARD_RETURN_5D\n",
    "    FROM ALPHA_SIGNALS\n",
    "),\n",
    "daily_returns AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL > 0 THEN FORWARD_RETURN_5D END) AS LONG_RETURN,\n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL < 0 THEN FORWARD_RETURN_5D END) AS SHORT_RETURN,\n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL > 0 THEN FORWARD_RETURN_5D END) - \n",
    "        AVG(CASE WHEN WEIGHTED_SIGNAL < 0 THEN FORWARD_RETURN_5D END) AS LONG_SHORT_RETURN\n",
    "    FROM signals_with_returns\n",
    "    WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    "    GROUP BY DATE\n",
    "    HAVING LONG_RETURN IS NOT NULL AND SHORT_RETURN IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    DATE,\n",
    "    LONG_RETURN,\n",
    "    SHORT_RETURN,\n",
    "    LONG_SHORT_RETURN,\n",
    "    SUM(LONG_SHORT_RETURN) OVER (ORDER BY DATE) AS CUMULATIVE_RETURN\n",
    "FROM daily_returns\n",
    "ORDER BY DATE\n",
    "\"\"\"\n",
    "\n",
    "# Get data for plotting\n",
    "cumulative_data = session.sql(cumulative_sql).to_pandas()\n",
    "cumulative_data['DATE'] = pd.to_datetime(cumulative_data['DATE'])\n",
    "cumulative_data['CUMULATIVE_PCT'] = cumulative_data['CUMULATIVE_RETURN'] * 100\n",
    "\n",
    "# Plot cumulative returns\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cumulative_data['DATE'],\n",
    "    y=cumulative_data['CUMULATIVE_PCT'],\n",
    "    mode='lines',\n",
    "    name='Long-Short Cumulative Return',\n",
    "    line=dict(color='#00C853', width=2),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(0, 200, 83, 0.2)'\n",
    "))\n",
    "\n",
    "# Add zero line\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ðŸ“ˆ Cumulative Long-Short Portfolio Returns',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Cumulative Return (%)',\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Summary stats\n",
    "total_return = cumulative_data['CUMULATIVE_PCT'].iloc[-1] if len(cumulative_data) > 0 else 0\n",
    "print(f\"ðŸ“Š Total Cumulative Return: {total_return:.2f}%\")\n",
    "print(f\"ðŸ“… Period: {cumulative_data['DATE'].min().strftime('%Y-%m-%d')} to {cumulative_data['DATE'].max().strftime('%Y-%m-%d')}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell35"
   },
   "source": [
    "### ðŸ”— Pairs Trading Analysis\n",
    "\n",
    "**Pairs Trading Strategy:**\n",
    "1. Find two stocks that are **highly correlated** (move together)\n",
    "2. Detect when they **diverge** (unusual gap opens)\n",
    "3. **SHORT** the outperformer, **LONG** the underperformer\n",
    "4. Profit when they **converge** back to normal\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell36",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate Correlation Matrix between all stocks\n",
    "correlation_sql = \"\"\"\n",
    "WITH daily_returns AS (\n",
    "    -- Calculate daily returns for each stock\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        CLOSE,\n",
    "        (CLOSE - LAG(CLOSE) OVER (PARTITION BY SYMBOL ORDER BY DATE)) \n",
    "            / NULLIF(LAG(CLOSE) OVER (PARTITION BY SYMBOL ORDER BY DATE), 0) AS DAILY_RETURN\n",
    "    FROM MARKET_DATA\n",
    ")\n",
    "SELECT \n",
    "    a.SYMBOL AS SYMBOL_1,\n",
    "    b.SYMBOL AS SYMBOL_2,\n",
    "    ROUND(CORR(a.DAILY_RETURN, b.DAILY_RETURN), 3) AS CORRELATION,\n",
    "    COUNT(*) AS TRADING_DAYS\n",
    "FROM daily_returns a\n",
    "JOIN daily_returns b \n",
    "    ON a.DATE = b.DATE \n",
    "    AND a.SYMBOL < b.SYMBOL  -- Avoid duplicates (A-B, not B-A)\n",
    "WHERE a.DAILY_RETURN IS NOT NULL \n",
    "  AND b.DAILY_RETURN IS NOT NULL\n",
    "GROUP BY a.SYMBOL, b.SYMBOL\n",
    "HAVING COUNT(*) >= 50  -- Need enough data points\n",
    "ORDER BY CORRELATION DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ”— STOCK CORRELATION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Highly correlated pairs are candidates for pairs trading\")\n",
    "print(\"=\" * 60)\n",
    "correlation_df = session.sql(correlation_sql).to_pandas()\n",
    "correlation_df.head(15)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000035"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell37",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Visualize Correlation Heatmap\n",
    "# Pivot to matrix format\n",
    "symbols = list(set(correlation_df['SYMBOL_1'].tolist() + correlation_df['SYMBOL_2'].tolist()))\n",
    "symbols.sort()\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = pd.DataFrame(index=symbols, columns=symbols, data=1.0)  # Diagonal = 1\n",
    "for _, row in correlation_df.iterrows():\n",
    "    corr_matrix.loc[row['SYMBOL_1'], row['SYMBOL_2']] = row['CORRELATION']\n",
    "    corr_matrix.loc[row['SYMBOL_2'], row['SYMBOL_1']] = row['CORRELATION']\n",
    "\n",
    "# Plot heatmap\n",
    "fig = px.imshow(\n",
    "    corr_matrix.astype(float),\n",
    "    labels=dict(color=\"Correlation\"),\n",
    "    x=symbols,\n",
    "    y=symbols,\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    title='ðŸ”— Stock Correlation Heatmap (Pairs Trading Candidates)'\n",
    ")\n",
    "fig.update_layout(width=700, height=600)\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Show top correlated pairs\n",
    "print(\"\\nðŸŽ¯ TOP CORRELATED PAIRS (Best for Pairs Trading):\")\n",
    "print(\"=\" * 50)\n",
    "top_pairs = correlation_df[correlation_df['CORRELATION'] > 0.5].head(10)\n",
    "for _, row in top_pairs.iterrows():\n",
    "    print(f\"   {row['SYMBOL_1']} â†” {row['SYMBOL_2']}: {row['CORRELATION']:.3f}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000036"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell38",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Step 2: Detect Divergences in HIGHLY CORRELATED Pairs Only\n",
    "# Key insight: Only look for divergences in pairs that REALLY move together!\n",
    "# Best pairs: Same sector, same business model, high correlation (>= 0.70)\n",
    "# NOW USING ASOF JOIN for robust 20-day lookback!\n",
    "\n",
    "# Define sector mappings for filtering\n",
    "SECTOR_PAIRS = \"\"\"\n",
    "-- Define which stocks are in the same sector (best for pairs trading)\n",
    "SELECT 'AAPL' AS SYMBOL, 'Tech' AS SECTOR UNION ALL\n",
    "SELECT 'GOOGL', 'Tech' UNION ALL SELECT 'MSFT', 'Tech' UNION ALL\n",
    "SELECT 'AMZN', 'Tech' UNION ALL SELECT 'META', 'Tech' UNION ALL\n",
    "SELECT 'NVDA', 'Tech' UNION ALL SELECT 'TSLA', 'Tech' UNION ALL\n",
    "SELECT 'JPM', 'Financials' UNION ALL SELECT 'GS', 'Financials' UNION ALL\n",
    "SELECT 'MS', 'Financials' UNION ALL SELECT 'BAC', 'Financials' UNION ALL\n",
    "SELECT 'WFC', 'Financials' UNION ALL SELECT 'C', 'Financials' UNION ALL\n",
    "SELECT 'V', 'Payments' UNION ALL SELECT 'MA', 'Payments' UNION ALL\n",
    "SELECT 'PYPL', 'Payments' UNION ALL SELECT 'SQ', 'Payments' UNION ALL\n",
    "SELECT 'JNJ', 'Healthcare' UNION ALL SELECT 'UNH', 'Healthcare' UNION ALL\n",
    "SELECT 'PFE', 'Healthcare' UNION ALL SELECT 'MRK', 'Healthcare' UNION ALL\n",
    "SELECT 'ABBV', 'Healthcare' UNION ALL\n",
    "SELECT 'WMT', 'Consumer' UNION ALL SELECT 'COST', 'Consumer' UNION ALL\n",
    "SELECT 'HD', 'Consumer' UNION ALL SELECT 'NKE', 'Consumer' UNION ALL\n",
    "SELECT 'SBUX', 'Consumer' UNION ALL\n",
    "SELECT 'XOM', 'Energy' UNION ALL SELECT 'CVX', 'Energy' UNION ALL\n",
    "SELECT 'COP', 'Energy' UNION ALL\n",
    "SELECT 'CAT', 'Industrials' UNION ALL SELECT 'BA', 'Industrials' UNION ALL\n",
    "SELECT 'UPS', 'Industrials' UNION ALL SELECT 'HON', 'Industrials'\n",
    "\"\"\"\n",
    "\n",
    "pairs_divergence_sql = f\"\"\"\n",
    "WITH sectors AS (\n",
    "    {SECTOR_PAIRS}\n",
    "),\n",
    "-- Step 1: Get current prices\n",
    "current_prices AS (\n",
    "    SELECT DATE, SYMBOL, CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "-- Step 2: Historical prices for ASOF JOIN\n",
    "historical_prices AS (\n",
    "    SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS HIST_CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "-- Step 3: Use ASOF JOIN to find price ~20 trading days ago (28 calendar days)\n",
    "returns_with_asof AS (\n",
    "    SELECT \n",
    "        c.DATE,\n",
    "        c.SYMBOL,\n",
    "        c.CLOSE,\n",
    "        h.HIST_DATE,\n",
    "        h.HIST_CLOSE AS CLOSE_20D_AGO,\n",
    "        -- Daily return (still use LAG for previous day - it's fine for this)\n",
    "        (c.CLOSE - LAG(c.CLOSE) OVER (PARTITION BY c.SYMBOL ORDER BY c.DATE)) \n",
    "            / NULLIF(LAG(c.CLOSE) OVER (PARTITION BY c.SYMBOL ORDER BY c.DATE), 0) AS DAILY_RETURN,\n",
    "        -- 20-day return using ASOF JOIN (more robust!)\n",
    "        (c.CLOSE - h.HIST_CLOSE) / NULLIF(h.HIST_CLOSE, 0) AS RETURN_20D\n",
    "    FROM current_prices c\n",
    "    ASOF JOIN historical_prices h\n",
    "        MATCH_CONDITION (DATEADD('day', -28, c.DATE) >= h.HIST_DATE)\n",
    "        ON c.SYMBOL = h.SYMBOL\n",
    "),\n",
    "daily_returns AS (\n",
    "    SELECT * FROM returns_with_asof\n",
    "    WHERE RETURN_20D IS NOT NULL\n",
    "),\n",
    "-- FIRST: Calculate correlations for SAME-SECTOR pairs with HIGH correlation\n",
    "pair_correlations AS (\n",
    "    SELECT \n",
    "        a.SYMBOL AS SYMBOL_1,\n",
    "        b.SYMBOL AS SYMBOL_2,\n",
    "        s1.SECTOR AS SECTOR,\n",
    "        CORR(a.DAILY_RETURN, b.DAILY_RETURN) AS CORRELATION\n",
    "    FROM daily_returns a\n",
    "    JOIN daily_returns b \n",
    "        ON a.DATE = b.DATE \n",
    "        AND a.SYMBOL < b.SYMBOL\n",
    "    -- Join sector info\n",
    "    JOIN sectors s1 ON a.SYMBOL = s1.SYMBOL\n",
    "    JOIN sectors s2 ON b.SYMBOL = s2.SYMBOL\n",
    "    WHERE a.DAILY_RETURN IS NOT NULL \n",
    "      AND b.DAILY_RETURN IS NOT NULL\n",
    "      AND s1.SECTOR = s2.SECTOR  -- âš ï¸ SAME SECTOR ONLY!\n",
    "    GROUP BY a.SYMBOL, b.SYMBOL, s1.SECTOR\n",
    "    HAVING CORR(a.DAILY_RETURN, b.DAILY_RETURN) >= 0.65  -- âš ï¸ Higher threshold!\n",
    "),\n",
    "-- THEN: Analyze only the same-sector correlated pairs\n",
    "pair_analysis AS (\n",
    "    SELECT \n",
    "        a.DATE,\n",
    "        a.SYMBOL AS SYMBOL_1,\n",
    "        b.SYMBOL AS SYMBOL_2,\n",
    "        pc.SECTOR,  -- Include sector for reference\n",
    "        pc.CORRELATION,  -- Include correlation for reference\n",
    "        a.CLOSE AS CLOSE_1,\n",
    "        b.CLOSE AS CLOSE_2,\n",
    "        a.RETURN_20D AS RETURN_20D_1,\n",
    "        b.RETURN_20D AS RETURN_20D_2,\n",
    "        -- Spread = difference in 20-day returns\n",
    "        a.RETURN_20D - b.RETURN_20D AS RETURN_SPREAD,\n",
    "        -- Normalized price ratio\n",
    "        a.CLOSE / NULLIF(b.CLOSE, 0) AS PRICE_RATIO\n",
    "    FROM daily_returns a\n",
    "    JOIN daily_returns b \n",
    "        ON a.DATE = b.DATE \n",
    "        AND a.SYMBOL < b.SYMBOL\n",
    "    -- âš ï¸ KEY FIX: Only join pairs that passed the sector + correlation filter!\n",
    "    JOIN pair_correlations pc \n",
    "        ON a.SYMBOL = pc.SYMBOL_1 \n",
    "        AND b.SYMBOL = pc.SYMBOL_2\n",
    "    WHERE a.RETURN_20D IS NOT NULL \n",
    "      AND b.RETURN_20D IS NOT NULL\n",
    "),\n",
    "-- Calculate spread statistics\n",
    "spread_stats AS (\n",
    "    SELECT \n",
    "        SYMBOL_1,\n",
    "        SYMBOL_2,\n",
    "        SECTOR,\n",
    "        CORRELATION,\n",
    "        DATE,\n",
    "        CLOSE_1,\n",
    "        CLOSE_2,\n",
    "        RETURN_20D_1,\n",
    "        RETURN_20D_2,\n",
    "        RETURN_SPREAD,\n",
    "        PRICE_RATIO,\n",
    "        AVG(RETURN_SPREAD) OVER (PARTITION BY SYMBOL_1, SYMBOL_2 ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) AS SPREAD_MEAN,\n",
    "        STDDEV(RETURN_SPREAD) OVER (PARTITION BY SYMBOL_1, SYMBOL_2 ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) AS SPREAD_STD\n",
    "    FROM pair_analysis\n",
    "),\n",
    "-- Calculate Z-score (how many std deviations from mean)\n",
    "divergence_signals AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        (RETURN_SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) AS ZSCORE,\n",
    "        CASE \n",
    "            WHEN (RETURN_SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) > 2 THEN 'SHORT_1_LONG_2'\n",
    "            WHEN (RETURN_SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) < -2 THEN 'LONG_1_SHORT_2'\n",
    "            ELSE 'NO_SIGNAL'\n",
    "        END AS PAIRS_SIGNAL\n",
    "    FROM spread_stats\n",
    "    WHERE SPREAD_STD IS NOT NULL AND SPREAD_STD > 0\n",
    ")\n",
    "SELECT \n",
    "    DATE,\n",
    "    SECTOR,  -- Show which sector\n",
    "    SYMBOL_1,\n",
    "    SYMBOL_2,\n",
    "    ROUND(CORRELATION, 2) AS CORR,  -- Show correlation!\n",
    "    ROUND(RETURN_20D_1 * 100, 2) AS RETURN_1_PCT,\n",
    "    ROUND(RETURN_20D_2 * 100, 2) AS RETURN_2_PCT,\n",
    "    ROUND(RETURN_SPREAD * 100, 2) AS SPREAD_PCT,\n",
    "    ROUND(ZSCORE, 2) AS ZSCORE,\n",
    "    PAIRS_SIGNAL\n",
    "FROM divergence_signals\n",
    "WHERE DATE = (SELECT MAX(DATE) FROM divergence_signals)\n",
    "ORDER BY CORRELATION DESC, ABS(ZSCORE) DESC  -- Show ALL pairs, sorted by correlation\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š ALL CORRELATED PAIRS STATUS (Same-Sector, Correlation >= 0.65)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âš ï¸  FILTERS: Same sector only, Correlation >= 0.65\")\n",
    "print(\"-\" * 80)\n",
    "divergence_df = session.sql(pairs_divergence_sql).to_pandas()\n",
    "\n",
    "# Separate into divergent and non-divergent\n",
    "divergent = divergence_df[abs(divergence_df['ZSCORE']) >= 2]\n",
    "approaching = divergence_df[(abs(divergence_df['ZSCORE']) >= 1.5) & (abs(divergence_df['ZSCORE']) < 2)]\n",
    "stable = divergence_df[abs(divergence_df['ZSCORE']) < 1.5]\n",
    "\n",
    "print(f\"\\nðŸ”´ DIVERGENT (|Z| >= 2) - TRADE NOW: {len(divergent)} pairs\")\n",
    "if len(divergent) > 0:\n",
    "    for _, row in divergent.iterrows():\n",
    "        action = \"SHORT \" + row['SYMBOL_1'] + \", LONG \" + row['SYMBOL_2'] if row['ZSCORE'] > 0 else \"LONG \" + row['SYMBOL_1'] + \", SHORT \" + row['SYMBOL_2']\n",
    "        print(f\"   {row['SECTOR']:12} â”‚ {row['SYMBOL_1']:5}â†”{row['SYMBOL_2']:5} â”‚ Corr: {row['CORR']:.2f} â”‚ Z: {row['ZSCORE']:+.2f} â”‚ {action}\")\n",
    "\n",
    "print(f\"\\nðŸŸ¡ APPROACHING (|Z| 1.5-2) - WATCH: {len(approaching)} pairs\")\n",
    "if len(approaching) > 0:\n",
    "    for _, row in approaching.head(5).iterrows():\n",
    "        print(f\"   {row['SECTOR']:12} â”‚ {row['SYMBOL_1']:5}â†”{row['SYMBOL_2']:5} â”‚ Corr: {row['CORR']:.2f} â”‚ Z: {row['ZSCORE']:+.2f}\")\n",
    "\n",
    "print(f\"\\nðŸŸ¢ STABLE (|Z| < 1.5) - NO ACTION: {len(stable)} pairs\")\n",
    "if len(stable) > 0:\n",
    "    # Show top 5 by correlation\n",
    "    top_stable = stable.nlargest(5, 'CORR')\n",
    "    for _, row in top_stable.iterrows():\n",
    "        print(f\"   {row['SECTOR']:12} â”‚ {row['SYMBOL_1']:5}â†”{row['SYMBOL_2']:5} â”‚ Corr: {row['CORR']:.2f} â”‚ Z: {row['ZSCORE']:+.2f} â”‚ Moving together âœ…\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "divergence_df\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000037"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell39",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Step 3: Visualize a Pair's Spread Over Time\n",
    "# Pick the top correlated pair and show when divergences occur\n",
    "# NOW USING ASOF JOIN for robust 20-day lookback!\n",
    "\n",
    "# Get the top correlated pair\n",
    "if len(correlation_df) > 0:\n",
    "    top_pair = correlation_df.iloc[0]\n",
    "    symbol_1 = top_pair['SYMBOL_1']\n",
    "    symbol_2 = top_pair['SYMBOL_2']\n",
    "    \n",
    "    spread_history_sql = f\"\"\"\n",
    "    WITH current_prices AS (\n",
    "        SELECT DATE, SYMBOL, CLOSE\n",
    "        FROM MARKET_DATA\n",
    "        WHERE SYMBOL IN ('{symbol_1}', '{symbol_2}')\n",
    "    ),\n",
    "    historical_prices AS (\n",
    "        SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS HIST_CLOSE\n",
    "        FROM MARKET_DATA\n",
    "        WHERE SYMBOL IN ('{symbol_1}', '{symbol_2}')\n",
    "    ),\n",
    "    -- Use ASOF JOIN for 20-day lookback\n",
    "    prices AS (\n",
    "        SELECT \n",
    "            c.DATE,\n",
    "            c.SYMBOL,\n",
    "            c.CLOSE,\n",
    "            (c.CLOSE - h.HIST_CLOSE) / NULLIF(h.HIST_CLOSE, 0) AS RETURN_20D\n",
    "        FROM current_prices c\n",
    "        ASOF JOIN historical_prices h\n",
    "            MATCH_CONDITION (DATEADD('day', -28, c.DATE) >= h.HIST_DATE)\n",
    "            ON c.SYMBOL = h.SYMBOL\n",
    "    ),\n",
    "    paired AS (\n",
    "        SELECT \n",
    "            a.DATE,\n",
    "            a.CLOSE AS CLOSE_1,\n",
    "            b.CLOSE AS CLOSE_2,\n",
    "            a.RETURN_20D AS RETURN_1,\n",
    "            b.RETURN_20D AS RETURN_2,\n",
    "            a.RETURN_20D - b.RETURN_20D AS SPREAD\n",
    "        FROM prices a\n",
    "        JOIN prices b ON a.DATE = b.DATE\n",
    "        WHERE a.SYMBOL = '{symbol_1}' AND b.SYMBOL = '{symbol_2}'\n",
    "    )\n",
    "    SELECT \n",
    "        DATE,\n",
    "        CLOSE_1,\n",
    "        CLOSE_2,\n",
    "        SPREAD,\n",
    "        AVG(SPREAD) OVER (ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) AS SPREAD_MEAN,\n",
    "        STDDEV(SPREAD) OVER (ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) AS SPREAD_STD,\n",
    "        (SPREAD - AVG(SPREAD) OVER (ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW)) \n",
    "            / NULLIF(STDDEV(SPREAD) OVER (ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW), 0) AS ZSCORE\n",
    "    FROM paired\n",
    "    WHERE SPREAD IS NOT NULL\n",
    "    ORDER BY DATE\n",
    "    \"\"\"\n",
    "    \n",
    "    spread_data = session.sql(spread_history_sql).to_pandas()\n",
    "    spread_data['DATE'] = pd.to_datetime(spread_data['DATE'])\n",
    "    \n",
    "    # Create subplot with prices and z-score\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(f'ðŸ“ˆ Price Comparison: {symbol_1} vs {symbol_2}', \n",
    "                       f'ðŸ“Š Spread Z-Score (Trading Signal)'),\n",
    "        row_heights=[0.5, 0.5],\n",
    "        vertical_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Normalize prices to 100 for comparison\n",
    "    spread_data['PRICE_1_NORM'] = spread_data['CLOSE_1'] / spread_data['CLOSE_1'].iloc[0] * 100\n",
    "    spread_data['PRICE_2_NORM'] = spread_data['CLOSE_2'] / spread_data['CLOSE_2'].iloc[0] * 100\n",
    "    \n",
    "    # Plot normalized prices\n",
    "    fig.add_trace(go.Scatter(x=spread_data['DATE'], y=spread_data['PRICE_1_NORM'], \n",
    "                             name=symbol_1, line=dict(color='#2196F3')), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=spread_data['DATE'], y=spread_data['PRICE_2_NORM'], \n",
    "                             name=symbol_2, line=dict(color='#FF9800')), row=1, col=1)\n",
    "    \n",
    "    # Plot Z-Score with trading bands\n",
    "    fig.add_trace(go.Scatter(x=spread_data['DATE'], y=spread_data['ZSCORE'], \n",
    "                             name='Z-Score', line=dict(color='#4CAF50')), row=2, col=1)\n",
    "    \n",
    "    # Add trading bands at +/- 2 standard deviations\n",
    "    fig.add_hline(y=2, line_dash=\"dash\", line_color=\"red\", row=2, col=1, \n",
    "                  annotation_text=\"SHORT Signal (+2Ïƒ)\")\n",
    "    fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"green\", row=2, col=1,\n",
    "                  annotation_text=\"LONG Signal (-2Ïƒ)\")\n",
    "    fig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\", row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        title_text=f\"ðŸ”— Pairs Trading: {symbol_1} â†” {symbol_2} (Correlation: {top_pair['CORRELATION']:.2f})\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Normalized Price (Base 100)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Z-Score\", row=2, col=1)\n",
    "    \n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š PAIRS TRADING INTERPRETATION:\")\n",
    "    print(f\"   â€¢ When Z-Score > +2: {symbol_1} outperformed â†’ SHORT {symbol_1}, LONG {symbol_2}\")\n",
    "    print(f\"   â€¢ When Z-Score < -2: {symbol_2} outperformed â†’ LONG {symbol_1}, SHORT {symbol_2}\")\n",
    "    print(f\"   â€¢ Exit when Z-Score returns to 0 (convergence)\")\n",
    "else:\n",
    "    print(\"No correlation data available\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell40",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Step 4: Pairs Trading Backtest\n",
    "# How profitable would pairs trading have been?\n",
    "# NOW USING ASOF JOIN for robust 20-day lookback!\n",
    "\n",
    "pairs_backtest_sql = \"\"\"\n",
    "WITH current_prices AS (\n",
    "    SELECT DATE, SYMBOL, CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "historical_prices AS (\n",
    "    SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS HIST_CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "-- Use ASOF JOIN for 20-day return, LEAD for forward return\n",
    "daily_returns AS (\n",
    "    SELECT \n",
    "        c.DATE,\n",
    "        c.SYMBOL,\n",
    "        c.CLOSE,\n",
    "        LEAD(c.CLOSE, 5) OVER (PARTITION BY c.SYMBOL ORDER BY c.DATE) AS CLOSE_5D_FORWARD,\n",
    "        (LEAD(c.CLOSE, 5) OVER (PARTITION BY c.SYMBOL ORDER BY c.DATE) - c.CLOSE) / c.CLOSE AS FORWARD_RETURN_5D,\n",
    "        (c.CLOSE - h.HIST_CLOSE) / NULLIF(h.HIST_CLOSE, 0) AS RETURN_20D\n",
    "    FROM current_prices c\n",
    "    ASOF JOIN historical_prices h\n",
    "        MATCH_CONDITION (DATEADD('day', -28, c.DATE) >= h.HIST_DATE)\n",
    "        ON c.SYMBOL = h.SYMBOL\n",
    "),\n",
    "pair_signals AS (\n",
    "    SELECT \n",
    "        a.DATE,\n",
    "        a.SYMBOL AS SYMBOL_1,\n",
    "        b.SYMBOL AS SYMBOL_2,\n",
    "        a.RETURN_20D - b.RETURN_20D AS SPREAD,\n",
    "        a.FORWARD_RETURN_5D AS FWD_RETURN_1,\n",
    "        b.FORWARD_RETURN_5D AS FWD_RETURN_2,\n",
    "        AVG(a.RETURN_20D - b.RETURN_20D) OVER (\n",
    "            PARTITION BY a.SYMBOL, b.SYMBOL ORDER BY a.DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "        ) AS SPREAD_MEAN,\n",
    "        STDDEV(a.RETURN_20D - b.RETURN_20D) OVER (\n",
    "            PARTITION BY a.SYMBOL, b.SYMBOL ORDER BY a.DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "        ) AS SPREAD_STD\n",
    "    FROM daily_returns a\n",
    "    JOIN daily_returns b ON a.DATE = b.DATE AND a.SYMBOL < b.SYMBOL\n",
    "    WHERE a.RETURN_20D IS NOT NULL AND b.RETURN_20D IS NOT NULL\n",
    "      AND a.FORWARD_RETURN_5D IS NOT NULL AND b.FORWARD_RETURN_5D IS NOT NULL\n",
    "),\n",
    "signals_with_zscore AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) AS ZSCORE,\n",
    "        CASE \n",
    "            WHEN (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) > 2 THEN \n",
    "                -FWD_RETURN_1 + FWD_RETURN_2  -- SHORT 1, LONG 2\n",
    "            WHEN (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) < -2 THEN \n",
    "                FWD_RETURN_1 - FWD_RETURN_2   -- LONG 1, SHORT 2\n",
    "            ELSE NULL\n",
    "        END AS PAIRS_TRADE_RETURN\n",
    "    FROM pair_signals\n",
    "    WHERE SPREAD_STD > 0\n",
    ")\n",
    "SELECT \n",
    "    'Pairs Trading Strategy' AS STRATEGY,\n",
    "    COUNT(PAIRS_TRADE_RETURN) AS NUM_TRADES,\n",
    "    ROUND(AVG(PAIRS_TRADE_RETURN) * 100, 3) AS AVG_RETURN_PCT,\n",
    "    ROUND(STDDEV(PAIRS_TRADE_RETURN) * 100, 3) AS VOLATILITY_PCT,\n",
    "    ROUND(AVG(PAIRS_TRADE_RETURN) / NULLIF(STDDEV(PAIRS_TRADE_RETURN), 0) * SQRT(52), 2) AS SHARPE_RATIO,\n",
    "    ROUND(SUM(CASE WHEN PAIRS_TRADE_RETURN > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS WIN_RATE_PCT\n",
    "FROM signals_with_zscore\n",
    "WHERE PAIRS_TRADE_RETURN IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š PAIRS TRADING BACKTEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Strategy: Enter when Z-Score > 2 or < -2, hold for 5 days\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(pairs_backtest_sql).show()\n",
    "\n",
    "print(\"\\nðŸ’¡ PAIRS TRADING vs MOMENTUM LONG-SHORT:\")\n",
    "print(\"   â€¢ Pairs Trading: Bets on CONVERGENCE (mean reversion)\")\n",
    "print(\"   â€¢ Momentum L/S:  Bets on CONTINUATION (trend following)\")\n",
    "print(\"   â€¢ Best practice: Combine both for diversification!\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000039"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell41"
   },
   "source": [
    "### ðŸ”¬ Deep Dive: Sector-Based Pair Analysis\n",
    "\n",
    "With our expanded stock universe (30+ stocks), we can now analyze pairs within sectors:\n",
    "- **Intra-sector pairs** (GSâ†”JPM, XOMâ†”CVX) tend to have higher correlation\n",
    "- **Cross-sector pairs** can provide diversification benefits\n",
    "- **Sector rotation** signals when entire sectors diverge\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell42",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Deep Dive: Best Pairs by Sector\n",
    "\n",
    "# Define sector mappings\n",
    "sector_mapping = {\n",
    "    # Tech Giants\n",
    "    'AAPL': 'Tech', 'GOOGL': 'Tech', 'MSFT': 'Tech', 'AMZN': 'Tech', \n",
    "    'META': 'Tech', 'NVDA': 'Tech', 'TSLA': 'Tech',\n",
    "    # Financials\n",
    "    'JPM': 'Financials', 'GS': 'Financials', 'MS': 'Financials', \n",
    "    'BAC': 'Financials', 'WFC': 'Financials', 'C': 'Financials',\n",
    "    # Payments\n",
    "    'V': 'Payments', 'MA': 'Payments', 'PYPL': 'Payments', 'SQ': 'Payments',\n",
    "    # Healthcare\n",
    "    'JNJ': 'Healthcare', 'UNH': 'Healthcare', 'PFE': 'Healthcare', \n",
    "    'MRK': 'Healthcare', 'ABBV': 'Healthcare',\n",
    "    # Consumer\n",
    "    'WMT': 'Consumer', 'COST': 'Consumer', 'HD': 'Consumer', \n",
    "    'NKE': 'Consumer', 'SBUX': 'Consumer',\n",
    "    # Energy\n",
    "    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy',\n",
    "    # Industrials\n",
    "    'CAT': 'Industrials', 'BA': 'Industrials', 'UPS': 'Industrials', 'HON': 'Industrials'\n",
    "}\n",
    "\n",
    "# Get correlations with sector labels\n",
    "sector_corr_sql = \"\"\"\n",
    "WITH daily_returns AS (\n",
    "    SELECT \n",
    "        DATE,\n",
    "        SYMBOL,\n",
    "        (CLOSE - LAG(CLOSE) OVER (PARTITION BY SYMBOL ORDER BY DATE)) \n",
    "            / NULLIF(LAG(CLOSE) OVER (PARTITION BY SYMBOL ORDER BY DATE), 0) AS DAILY_RETURN\n",
    "    FROM MARKET_DATA\n",
    ")\n",
    "SELECT \n",
    "    a.SYMBOL AS SYMBOL_1,\n",
    "    b.SYMBOL AS SYMBOL_2,\n",
    "    ROUND(CORR(a.DAILY_RETURN, b.DAILY_RETURN), 3) AS CORRELATION,\n",
    "    COUNT(*) AS TRADING_DAYS\n",
    "FROM daily_returns a\n",
    "JOIN daily_returns b \n",
    "    ON a.DATE = b.DATE \n",
    "    AND a.SYMBOL < b.SYMBOL\n",
    "WHERE a.DAILY_RETURN IS NOT NULL \n",
    "  AND b.DAILY_RETURN IS NOT NULL\n",
    "GROUP BY a.SYMBOL, b.SYMBOL\n",
    "HAVING COUNT(*) >= 50\n",
    "ORDER BY CORRELATION DESC\n",
    "\"\"\"\n",
    "\n",
    "sector_corr_df = session.sql(sector_corr_sql).to_pandas()\n",
    "\n",
    "# Add sector labels\n",
    "sector_corr_df['SECTOR_1'] = sector_corr_df['SYMBOL_1'].map(sector_mapping)\n",
    "sector_corr_df['SECTOR_2'] = sector_corr_df['SYMBOL_2'].map(sector_mapping)\n",
    "sector_corr_df['SAME_SECTOR'] = sector_corr_df['SECTOR_1'] == sector_corr_df['SECTOR_2']\n",
    "\n",
    "# Show best pairs by category\n",
    "print(\"ðŸ† TOP INTRA-SECTOR PAIRS (Same Industry)\")\n",
    "print(\"=\" * 70)\n",
    "intra_sector = sector_corr_df[sector_corr_df['SAME_SECTOR']].head(10)\n",
    "for _, row in intra_sector.iterrows():\n",
    "    print(f\"   {row['SECTOR_1']:12} â”‚ {row['SYMBOL_1']:5} â†” {row['SYMBOL_2']:5} â”‚ Correlation: {row['CORRELATION']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸŒ TOP CROSS-SECTOR PAIRS (Different Industries)\")\n",
    "print(\"=\" * 70)\n",
    "cross_sector = sector_corr_df[~sector_corr_df['SAME_SECTOR']].head(10)\n",
    "for _, row in cross_sector.iterrows():\n",
    "    print(f\"   {row['SECTOR_1']:12} vs {row['SECTOR_2']:12} â”‚ {row['SYMBOL_1']:5} â†” {row['SYMBOL_2']:5} â”‚ Correlation: {row['CORRELATION']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ INSIGHT:\")\n",
    "print(\"   â€¢ Intra-sector pairs: Higher correlation = more reliable mean reversion\")\n",
    "print(\"   â€¢ Cross-sector pairs: Unexpected correlations can reveal hidden connections\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000041"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell43",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Interactive Deep Dive: Select Best Pair from Each Sector\n",
    "# NOW USING ASOF JOIN for robust 20-day lookback!\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Get the best pair from each sector\n",
    "best_by_sector = intra_sector.groupby('SECTOR_1').first().reset_index()\n",
    "\n",
    "print(\"ðŸ“Š BEST PAIR FROM EACH SECTOR:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build subplot titles using iterrows (dictionary access)\n",
    "subplot_titles = []\n",
    "for _, row in best_by_sector.iterrows():\n",
    "    subplot_titles.append(f\"{row['SYMBOL_1']} vs {row['SYMBOL_2']} (Prices)\")\n",
    "    subplot_titles.append(\"Spread Z-Score\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(best_by_sector), cols=2,\n",
    "    subplot_titles=subplot_titles,\n",
    "    horizontal_spacing=0.1,\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "for idx, (_, pair) in enumerate(best_by_sector.iterrows()):\n",
    "    symbol_1, symbol_2 = pair['SYMBOL_1'], pair['SYMBOL_2']\n",
    "    \n",
    "    # Get spread data for this pair using ASOF JOIN\n",
    "    pair_sql = f\"\"\"\n",
    "    WITH current_prices AS (\n",
    "        SELECT DATE, SYMBOL, CLOSE\n",
    "        FROM MARKET_DATA\n",
    "        WHERE SYMBOL IN ('{symbol_1}', '{symbol_2}')\n",
    "    ),\n",
    "    historical_prices AS (\n",
    "        SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS HIST_CLOSE\n",
    "        FROM MARKET_DATA\n",
    "        WHERE SYMBOL IN ('{symbol_1}', '{symbol_2}')\n",
    "    ),\n",
    "    daily_returns AS (\n",
    "        SELECT \n",
    "            c.DATE, c.SYMBOL, c.CLOSE,\n",
    "            (c.CLOSE - h.HIST_CLOSE) / NULLIF(h.HIST_CLOSE, 0) AS RETURN_20D\n",
    "        FROM current_prices c\n",
    "        ASOF JOIN historical_prices h\n",
    "            MATCH_CONDITION (DATEADD('day', -28, c.DATE) >= h.HIST_DATE)\n",
    "            ON c.SYMBOL = h.SYMBOL\n",
    "    ),\n",
    "    pivoted AS (\n",
    "        SELECT a.DATE, \n",
    "               a.CLOSE AS CLOSE_1, a.RETURN_20D AS RETURN_1,\n",
    "               b.CLOSE AS CLOSE_2, b.RETURN_20D AS RETURN_2,\n",
    "               a.RETURN_20D - b.RETURN_20D AS SPREAD\n",
    "        FROM daily_returns a\n",
    "        JOIN daily_returns b ON a.DATE = b.DATE\n",
    "        WHERE a.SYMBOL = '{symbol_1}' AND b.SYMBOL = '{symbol_2}'\n",
    "    ),\n",
    "    with_stats AS (\n",
    "        SELECT *,\n",
    "            AVG(SPREAD) OVER (ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) AS SPREAD_MEAN,\n",
    "            STDDEV(SPREAD) OVER (ORDER BY DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW) AS SPREAD_STD\n",
    "        FROM pivoted\n",
    "    )\n",
    "    SELECT *, (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) AS ZSCORE\n",
    "    FROM with_stats\n",
    "    WHERE SPREAD_STD IS NOT NULL\n",
    "    ORDER BY DATE\n",
    "    \"\"\"\n",
    "    \n",
    "    pair_data = session.sql(pair_sql).to_pandas()\n",
    "    \n",
    "    if len(pair_data) > 0:\n",
    "        # Normalize prices\n",
    "        pair_data['PRICE_1_NORM'] = pair_data['CLOSE_1'] / pair_data['CLOSE_1'].iloc[0] * 100\n",
    "        pair_data['PRICE_2_NORM'] = pair_data['CLOSE_2'] / pair_data['CLOSE_2'].iloc[0] * 100\n",
    "        \n",
    "        row_num = idx + 1\n",
    "        \n",
    "        # Price chart\n",
    "        fig.add_trace(go.Scatter(x=pair_data['DATE'], y=pair_data['PRICE_1_NORM'],\n",
    "                                 name=symbol_1, line=dict(color='#2196F3', width=1.5),\n",
    "                                 showlegend=(idx==0)), row=row_num, col=1)\n",
    "        fig.add_trace(go.Scatter(x=pair_data['DATE'], y=pair_data['PRICE_2_NORM'],\n",
    "                                 name=symbol_2, line=dict(color='#FF9800', width=1.5),\n",
    "                                 showlegend=(idx==0)), row=row_num, col=1)\n",
    "        \n",
    "        # Z-Score chart with signal bands\n",
    "        fig.add_trace(go.Scatter(x=pair_data['DATE'], y=pair_data['ZSCORE'],\n",
    "                                 name='Z-Score', line=dict(color='#4CAF50', width=1),\n",
    "                                 fill='tozeroy', fillcolor='rgba(76, 175, 80, 0.2)',\n",
    "                                 showlegend=(idx==0)), row=row_num, col=2)\n",
    "        \n",
    "        # Add signal bands\n",
    "        fig.add_hline(y=2, line_dash=\"dash\", line_color=\"red\", line_width=0.5, row=row_num, col=2)\n",
    "        fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"green\", line_width=0.5, row=row_num, col=2)\n",
    "        \n",
    "        # Count signals\n",
    "        num_signals = len(pair_data[abs(pair_data['ZSCORE']) > 2])\n",
    "        print(f\"   {pair['SECTOR_1']:12} â”‚ {symbol_1} â†” {symbol_2} â”‚ Corr: {pair['CORRELATION']:.2f} â”‚ Signals: {num_signals}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=250 * len(best_by_sector),\n",
    "    title_text=\"ðŸ”¬ Sector-by-Sector Pair Analysis\",\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02)\n",
    ")\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell44",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Current Trading Opportunities - SAME SECTOR, HIGH CORRELATION PAIRS ONLY\n",
    "# This is the corrected version that only shows legitimate pairs trading opportunities\n",
    "# NOW USING ASOF JOIN for robust 20-day lookback!\n",
    "\n",
    "current_opportunities_sql = f\"\"\"\n",
    "WITH sectors AS (\n",
    "    {SECTOR_PAIRS}\n",
    "),\n",
    "current_prices AS (\n",
    "    SELECT DATE, SYMBOL, CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "historical_prices AS (\n",
    "    SELECT DATE AS HIST_DATE, SYMBOL, CLOSE AS HIST_CLOSE\n",
    "    FROM MARKET_DATA\n",
    "),\n",
    "-- Use ASOF JOIN for 20-day return (more robust than LAG)\n",
    "daily_returns AS (\n",
    "    SELECT \n",
    "        c.DATE,\n",
    "        c.SYMBOL,\n",
    "        c.CLOSE,\n",
    "        (c.CLOSE - LAG(c.CLOSE) OVER (PARTITION BY c.SYMBOL ORDER BY c.DATE)) \n",
    "            / NULLIF(LAG(c.CLOSE) OVER (PARTITION BY c.SYMBOL ORDER BY c.DATE), 0) AS DAILY_RETURN,\n",
    "        (c.CLOSE - h.HIST_CLOSE) / NULLIF(h.HIST_CLOSE, 0) AS RETURN_20D\n",
    "    FROM current_prices c\n",
    "    ASOF JOIN historical_prices h\n",
    "        MATCH_CONDITION (DATEADD('day', -28, c.DATE) >= h.HIST_DATE)\n",
    "        ON c.SYMBOL = h.SYMBOL\n",
    "),\n",
    "-- First filter: Same sector + high correlation pairs only\n",
    "pair_correlations AS (\n",
    "    SELECT \n",
    "        a.SYMBOL AS SYMBOL_1,\n",
    "        b.SYMBOL AS SYMBOL_2,\n",
    "        s1.SECTOR AS SECTOR,\n",
    "        CORR(a.DAILY_RETURN, b.DAILY_RETURN) AS CORRELATION\n",
    "    FROM daily_returns a\n",
    "    JOIN daily_returns b ON a.DATE = b.DATE AND a.SYMBOL < b.SYMBOL\n",
    "    JOIN sectors s1 ON a.SYMBOL = s1.SYMBOL\n",
    "    JOIN sectors s2 ON b.SYMBOL = s2.SYMBOL\n",
    "    WHERE a.DAILY_RETURN IS NOT NULL AND b.DAILY_RETURN IS NOT NULL\n",
    "      AND s1.SECTOR = s2.SECTOR  -- Same sector only!\n",
    "    GROUP BY a.SYMBOL, b.SYMBOL, s1.SECTOR\n",
    "    HAVING CORR(a.DAILY_RETURN, b.DAILY_RETURN) >= 0.65  -- High correlation only!\n",
    "),\n",
    "pair_analysis AS (\n",
    "    SELECT \n",
    "        a.DATE,\n",
    "        pc.SECTOR,\n",
    "        a.SYMBOL AS SYMBOL_1,\n",
    "        b.SYMBOL AS SYMBOL_2,\n",
    "        pc.CORRELATION,\n",
    "        a.RETURN_20D - b.RETURN_20D AS SPREAD,\n",
    "        AVG(a.RETURN_20D - b.RETURN_20D) OVER (\n",
    "            PARTITION BY a.SYMBOL, b.SYMBOL ORDER BY a.DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "        ) AS SPREAD_MEAN,\n",
    "        STDDEV(a.RETURN_20D - b.RETURN_20D) OVER (\n",
    "            PARTITION BY a.SYMBOL, b.SYMBOL ORDER BY a.DATE ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "        ) AS SPREAD_STD\n",
    "    FROM daily_returns a\n",
    "    JOIN daily_returns b ON a.DATE = b.DATE AND a.SYMBOL < b.SYMBOL\n",
    "    JOIN pair_correlations pc ON a.SYMBOL = pc.SYMBOL_1 AND b.SYMBOL = pc.SYMBOL_2\n",
    "    WHERE a.RETURN_20D IS NOT NULL AND b.RETURN_20D IS NOT NULL\n",
    "),\n",
    "latest_signals AS (\n",
    "    SELECT \n",
    "        SECTOR,\n",
    "        SYMBOL_1,\n",
    "        SYMBOL_2,\n",
    "        CORRELATION,\n",
    "        SPREAD,\n",
    "        (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) AS ZSCORE,\n",
    "        CASE \n",
    "            WHEN (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) > 2 THEN \n",
    "                'ðŸ”´ SHORT ' || SYMBOL_1 || ', LONG ' || SYMBOL_2\n",
    "            WHEN (SPREAD - SPREAD_MEAN) / NULLIF(SPREAD_STD, 0) < -2 THEN \n",
    "                'ðŸŸ¢ LONG ' || SYMBOL_1 || ', SHORT ' || SYMBOL_2\n",
    "            ELSE 'âšª No Signal'\n",
    "        END AS TRADE_ACTION\n",
    "    FROM pair_analysis\n",
    "    WHERE DATE = (SELECT MAX(DATE) FROM pair_analysis)\n",
    "      AND SPREAD_STD > 0\n",
    ")\n",
    "SELECT \n",
    "    SECTOR,\n",
    "    SYMBOL_1,\n",
    "    SYMBOL_2,\n",
    "    ROUND(CORRELATION, 2) AS CORR,\n",
    "    ROUND(SPREAD * 100, 2) AS SPREAD_PCT,\n",
    "    ROUND(ZSCORE, 2) AS ZSCORE,\n",
    "    TRADE_ACTION\n",
    "FROM latest_signals\n",
    "WHERE ABS(ZSCORE) > 1.5\n",
    "ORDER BY ABS(ZSCORE) DESC\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸŽ¯ CURRENT PAIRS TRADING OPPORTUNITIES (Same-Sector, High-Corr Only)\")\n",
    "print(\"=\" * 85)\n",
    "print(\"âš ï¸  FILTERS: Same sector + Correlation >= 0.65\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "opportunities_df = session.sql(current_opportunities_sql).to_pandas()\n",
    "\n",
    "if len(opportunities_df) > 0:\n",
    "    for _, row in opportunities_df.iterrows():\n",
    "        zscore_emoji = \"ðŸ”¥\" if abs(row['ZSCORE']) > 2.5 else \"ðŸ“Š\"\n",
    "        signal_color = \"ðŸ”´\" if row['ZSCORE'] > 0 else \"ðŸŸ¢\"\n",
    "        print(f\"{zscore_emoji} {row['SECTOR']:12} â”‚ {row['SYMBOL_1']:5}â†”{row['SYMBOL_2']:5} â”‚ Corr: {row['CORR']:.2f} â”‚ Z: {row['ZSCORE']:+6.2f} â”‚ {row['TRADE_ACTION']}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ SUMMARY:\")\n",
    "    strong_signals = len(opportunities_df[abs(opportunities_df['ZSCORE']) > 2])\n",
    "    print(f\"   â€¢ Strong signals (|Z| > 2): {strong_signals}\")\n",
    "    print(f\"   â€¢ Moderate signals (|Z| > 1.5): {len(opportunities_df)}\")\n",
    "    print(f\"   â€¢ All pairs are same-sector with correlation >= 0.65 âœ…\")\n",
    "else:\n",
    "    print(\"   âœ… No divergent pairs found among same-sector correlated stocks\")\n",
    "    print(\"   (All sector peers are moving together as expected)\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000043"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell45",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Factor-level Information Coefficients\n",
    "factor_ic_sql = \"\"\"\n",
    "WITH signals_with_returns AS (\n",
    "    SELECT \n",
    "        a.*,\n",
    "        (LEAD(CLOSE, 5) OVER (PARTITION BY SYMBOL ORDER BY DATE) - CLOSE) / CLOSE AS FORWARD_RETURN_5D\n",
    "    FROM ALPHA_SIGNALS a\n",
    ")\n",
    "SELECT \n",
    "    'Momentum' AS FACTOR, ROUND(CORR(MOMENTUM_RANK, FORWARD_RETURN_5D), 4) AS IC\n",
    "FROM signals_with_returns WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    "UNION ALL\n",
    "SELECT 'Volatility', ROUND(CORR(VOLATILITY_RANK, FORWARD_RETURN_5D), 4)\n",
    "FROM signals_with_returns WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    "UNION ALL\n",
    "SELECT 'Sentiment', ROUND(CORR(SENTIMENT_RANK, FORWARD_RETURN_5D), 4)\n",
    "FROM signals_with_returns WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    "UNION ALL\n",
    "SELECT 'Composite', ROUND(CORR(COMPOSITE_ALPHA, FORWARD_RETURN_5D), 4)\n",
    "FROM signals_with_returns WHERE FORWARD_RETURN_5D IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š FACTOR INFORMATION COEFFICIENTS\")\n",
    "print(\"(Higher IC = better predictive power)\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(factor_ic_sql).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000044"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell46"
   },
   "source": [
    "---\n",
    "## 8ï¸âƒ£ Production Deployment\n",
    "\n",
    "Deploy this as a scheduled production pipeline using Snowflake Tasks and Stored Procedures.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell47",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create a UDF for composite alpha calculation (using IC-optimized weights)\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "@udf(name=\"COMPOSITE_ALPHA_UDF\", is_permanent=False, replace=True)\n",
    "def composite_alpha_udf(\n",
    "    momentum_rank: float, \n",
    "    volatility_rank: float, \n",
    "    sentiment_rank: float\n",
    ") -> float:\n",
    "    \"\"\"Calculate IC-optimized composite alpha score.\n",
    "    \n",
    "    Weights are based on IC analysis:\n",
    "    - Momentum: -0.20 (flipped - mean reversion)\n",
    "    - Volatility: -0.30 (flipped - high vol outperforming)\n",
    "    - Sentiment: +0.50 (most predictive factor)\n",
    "    \"\"\"\n",
    "    m = momentum_rank if momentum_rank is not None else 0.5\n",
    "    v = volatility_rank if volatility_rank is not None else 0.5\n",
    "    s = sentiment_rank if sentiment_rank is not None else 0.5\n",
    "    # IC-Optimized weights (negative = flipped signal)\n",
    "    return float(m * (-0.20) + v * (-0.30) + s * 0.50)\n",
    "\n",
    "print(\"âœ… Created COMPOSITE_ALPHA_UDF with IC-optimized weights\")\n",
    "print(\"   Momentum:   -20% (flipped)\")\n",
    "print(\"   Volatility: -30% (flipped)\")\n",
    "print(\"   Sentiment:  +50%\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000046"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell48",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Example: Create a stored procedure for daily alpha refresh\n",
    "# NOTE: In production, consider using ASOF JOIN instead of LAG(20) for robustness\n",
    "# See momentum factor cell for the ASOF JOIN pattern that handles data gaps\n",
    "stored_proc_sql = \"\"\"\n",
    "-- Stored Procedure for Daily Alpha Calculation\n",
    "-- Run this in your Snowflake console to create the procedure\n",
    "-- TIP: For production, use ASOF JOIN instead of LAG(20) for robust lookbacks\n",
    "\n",
    "CREATE OR REPLACE PROCEDURE CALCULATE_DAILY_ALPHA()\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "$$\n",
    "BEGIN\n",
    "    -- Step 1: Refresh Momentum Factor\n",
    "    CREATE OR REPLACE TABLE MOMENTUM_FACTOR AS\n",
    "    SELECT DATE, SYMBOL, CLOSE, MOMENTUM_20D, MOMENTUM_RANK, MOMENTUM_SIGNAL\n",
    "    FROM (\n",
    "        SELECT \n",
    "            DATE, SYMBOL, CLOSE,\n",
    "            (CLOSE - LAG(CLOSE, 20) OVER (PARTITION BY SYMBOL ORDER BY DATE)) \n",
    "                / NULLIF(LAG(CLOSE, 20) OVER (PARTITION BY SYMBOL ORDER BY DATE), 0) AS MOMENTUM_20D,\n",
    "            PERCENT_RANK() OVER (PARTITION BY DATE ORDER BY MOMENTUM_20D) AS MOMENTUM_RANK,\n",
    "            CASE WHEN MOMENTUM_RANK > 0.8 THEN 1 WHEN MOMENTUM_RANK < 0.2 THEN -1 ELSE 0 END AS MOMENTUM_SIGNAL\n",
    "        FROM MARKET_DATA\n",
    "    );\n",
    "    \n",
    "    -- Step 2: Combine factors and create signals\n",
    "    -- ... (similar logic to our composite query)\n",
    "    \n",
    "    RETURN 'Alpha calculation completed at ' || CURRENT_TIMESTAMP();\n",
    "END;\n",
    "$$;\n",
    "\n",
    "-- Schedule as a task (runs daily at 6 AM ET)\n",
    "CREATE OR REPLACE TASK DAILY_ALPHA_TASK\n",
    "    WAREHOUSE = COMPUTE_WH\n",
    "    SCHEDULE = 'USING CRON 0 6 * * * America/New_York'\n",
    "AS\n",
    "    CALL CALCULATE_DAILY_ALPHA();\n",
    "\n",
    "-- Resume the task (run once to enable)\n",
    "-- ALTER TASK DAILY_ALPHA_TASK RESUME;\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“‹ PRODUCTION DEPLOYMENT SQL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Copy the following SQL to create scheduled alpha calculations:\")\n",
    "print(stored_proc_sql)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000047"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell49"
   },
   "source": [
    "---\n",
    "## ðŸŽ“ Summary\n",
    "\n",
    "This notebook demonstrated how Snowflake enables hedge funds to **find alpha**:\n",
    "\n",
    "| Capability | Snowflake Feature | Benefit |\n",
    "|------------|-------------------|---------|\n",
    "| **Unified Data** | Single platform | No data silos, faster insights |\n",
    "| **Alternative Data** | Marketplace | Instant access to sentiment, news, web traffic |\n",
    "| **Alpha Computation** | Snowpark Python | ML/analytics without data movement |\n",
    "| **Scalability** | Elastic compute | Process billions of rows in minutes |\n",
    "| **Production ML** | UDFs + Tasks | Deploy factors as scalable pipelines |\n",
    "| **Governance** | Row-level security | Secure data sharing with LPs |\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "1. Connect real market data via **Snowpipe**\n",
    "2. Subscribe to **Marketplace** sentiment data (RavenPack, FactSet)\n",
    "3. Tune factor weights with **ML optimization**\n",
    "4. Deploy as **scheduled production pipeline**\n",
    "5. Set up **LP reporting shares**\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000048"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell50",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Show all created tables\n",
    "print(\"ðŸ“¦ TABLES CREATED IN THIS DEMO\")\n",
    "print(\"=\" * 60)\n",
    "session.sql(\"SHOW TABLES IN SCHEMA HEDGE_FUND_DEMO.ANALYTICS\").show()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Demo complete! Upload this notebook to Snowflake Notebooks to run it.\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000049"
  }
 ]
}